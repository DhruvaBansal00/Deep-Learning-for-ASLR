2020-08-09 02:11:40,938 Hello! This is Joey-NMT.
2020-08-09 02:11:42,025 Total params: 471050
2020-08-09 02:11:42,026 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'trg_embed.lut.weight']
2020-08-09 02:11:42,027 cfg.name                           : my_experiment
2020-08-09 02:11:42,027 cfg.data.src                       : data
2020-08-09 02:11:42,027 cfg.data.trg                       : en
2020-08-09 02:11:42,027 cfg.data.train                     : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/Transformers/test/data/asl/train
2020-08-09 02:11:42,027 cfg.data.dev                       : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/Transformers/test/data/asl/dev
2020-08-09 02:11:42,028 cfg.data.test                      : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/Transformers/test/data/asl/test
2020-08-09 02:11:42,028 cfg.data.level                     : word
2020-08-09 02:11:42,028 cfg.data.lowercase                 : True
2020-08-09 02:11:42,028 cfg.data.max_src_length            : 400
2020-08-09 02:11:42,028 cfg.data.max_trg_length            : 7
2020-08-09 02:11:42,028 cfg.data.src_voc_min_freq          : 1
2020-08-09 02:11:42,028 cfg.data.src_voc_limit             : 101
2020-08-09 02:11:42,028 cfg.data.trg_voc_min_freq          : 1
2020-08-09 02:11:42,028 cfg.data.trg_voc_limit             : 102
2020-08-09 02:11:42,028 cfg.testing.beam_size              : 5
2020-08-09 02:11:42,028 cfg.testing.alpha                  : 1.0
2020-08-09 02:11:42,028 cfg.training.reset_best_ckpt       : False
2020-08-09 02:11:42,028 cfg.training.reset_scheduler       : False
2020-08-09 02:11:42,028 cfg.training.reset_optimizer       : False
2020-08-09 02:11:42,028 cfg.training.random_seed           : 42
2020-08-09 02:11:42,028 cfg.training.optimizer             : adam
2020-08-09 02:11:42,029 cfg.training.adam_betas            : [0.9, 0.98]
2020-08-09 02:11:42,029 cfg.training.learning_rate         : 0.001
2020-08-09 02:11:42,029 cfg.training.learning_rate_min     : 1e-06
2020-08-09 02:11:42,029 cfg.training.learning_rate_factor  : 0.5
2020-08-09 02:11:42,029 cfg.training.learning_rate_warmup  : 1000
2020-08-09 02:11:42,029 cfg.training.weight_decay          : 0.0
2020-08-09 02:11:42,029 cfg.training.batch_size            : 250
2020-08-09 02:11:42,029 cfg.training.batch_type            : token
2020-08-09 02:11:42,029 cfg.training.batch_multiplier      : 1
2020-08-09 02:11:42,029 cfg.training.normalization         : batch
2020-08-09 02:11:42,029 cfg.training.scheduling            : noam
2020-08-09 02:11:42,029 cfg.training.epochs                : 100
2020-08-09 02:11:42,029 cfg.training.validation_freq       : 100
2020-08-09 02:11:42,029 cfg.training.logging_freq          : 10
2020-08-09 02:11:42,029 cfg.training.eval_metric           : sequence_accuracy
2020-08-09 02:11:42,029 cfg.training.early_stopping_metric : loss
2020-08-09 02:11:42,029 cfg.training.model_dir             : models/transformer
2020-08-09 02:11:42,030 cfg.training.overwrite             : True
2020-08-09 02:11:42,030 cfg.training.shuffle               : True
2020-08-09 02:11:42,030 cfg.training.use_cuda              : False
2020-08-09 02:11:42,030 cfg.training.max_output_length     : 31
2020-08-09 02:11:42,030 cfg.training.print_valid_sents     : [0, 1, 2]
2020-08-09 02:11:42,030 cfg.training.keep_last_ckpts       : 3
2020-08-09 02:11:42,030 cfg.training.label_smoothing       : 0.0
2020-08-09 02:11:42,030 cfg.model.initializer              : xavier
2020-08-09 02:11:42,030 cfg.model.init_gain                : 1.0
2020-08-09 02:11:42,030 cfg.model.bias_initializer         : zeros
2020-08-09 02:11:42,030 cfg.model.embed_initializer        : xavier
2020-08-09 02:11:42,030 cfg.model.embed_init_gain          : 1.0
2020-08-09 02:11:42,030 cfg.model.tied_embeddings          : False
2020-08-09 02:11:42,030 cfg.model.tied_softmax             : True
2020-08-09 02:11:42,030 cfg.model.encoder.type             : transformer
2020-08-09 02:11:42,030 cfg.model.encoder.num_layers       : 3
2020-08-09 02:11:42,030 cfg.model.encoder.num_heads        : 2
2020-08-09 02:11:42,031 cfg.model.encoder.embeddings.embedding_dim : 94
2020-08-09 02:11:42,031 cfg.model.encoder.embeddings.scale : True
2020-08-09 02:11:42,031 cfg.model.encoder.embeddings.freeze : False
2020-08-09 02:11:42,031 cfg.model.encoder.hidden_size      : 94
2020-08-09 02:11:42,031 cfg.model.encoder.ff_size          : 128
2020-08-09 02:11:42,031 cfg.model.encoder.dropout          : 0.1
2020-08-09 02:11:42,031 cfg.model.encoder.freeze           : False
2020-08-09 02:11:42,031 cfg.model.decoder.type             : transformer
2020-08-09 02:11:42,031 cfg.model.decoder.num_layers       : 3
2020-08-09 02:11:42,031 cfg.model.decoder.num_heads        : 2
2020-08-09 02:11:42,031 cfg.model.decoder.embeddings.embedding_dim : 94
2020-08-09 02:11:42,031 cfg.model.decoder.embeddings.scale : True
2020-08-09 02:11:42,031 cfg.model.decoder.embeddings.freeze : False
2020-08-09 02:11:42,031 cfg.model.decoder.hidden_size      : 94
2020-08-09 02:11:42,031 cfg.model.decoder.ff_size          : 128
2020-08-09 02:11:42,031 cfg.model.decoder.dropout          : 0.1
2020-08-09 02:11:42,031 cfg.model.decoder.freeze           : False
2020-08-09 02:11:42,032 Model(
	encoder=TransformerEncoder(num_layers=3, num_heads=2),
	decoder=TransformerDecoder(num_layers=3, num_heads=2),
	trg_embed=Embeddings(embedding_dim=94, vocab_size=7))
2020-08-09 02:11:42,034 EPOCH 1
2020-08-09 02:11:42,092 Epoch   1: total training loss 23.12
2020-08-09 02:11:42,092 EPOCH 2
2020-08-09 02:11:42,143 Epoch   2: total training loss 18.12
2020-08-09 02:11:42,144 EPOCH 3
2020-08-09 02:11:42,209 Epoch   3: total training loss 17.92
2020-08-09 02:11:42,209 EPOCH 4
2020-08-09 02:11:42,273 Epoch   4: total training loss 19.98
2020-08-09 02:11:42,273 EPOCH 5
2020-08-09 02:11:42,335 Epoch   5: total training loss 17.70
2020-08-09 02:11:42,335 EPOCH 6
2020-08-09 02:11:42,389 Epoch   6: total training loss 20.04
2020-08-09 02:11:42,390 EPOCH 7
2020-08-09 02:11:42,440 Epoch   7: total training loss 17.27
2020-08-09 02:11:42,440 EPOCH 8
2020-08-09 02:11:42,489 Epoch   8: total training loss 18.66
2020-08-09 02:11:42,489 EPOCH 9
2020-08-09 02:11:42,539 Epoch   9: total training loss 17.35
2020-08-09 02:11:42,539 EPOCH 10
2020-08-09 02:11:42,589 Epoch  10 Step:       10 Batch Loss:    18.675886 Tokens per Sec:       82, Lr: 0.000016
2020-08-09 02:11:42,589 Epoch  10: total training loss 18.68
2020-08-09 02:11:42,589 EPOCH 11
2020-08-09 02:11:42,638 Epoch  11: total training loss 16.74
2020-08-09 02:11:42,638 EPOCH 12
2020-08-09 02:11:42,687 Epoch  12: total training loss 15.67
2020-08-09 02:11:42,687 EPOCH 13
2020-08-09 02:11:42,734 Epoch  13: total training loss 18.98
2020-08-09 02:11:42,735 EPOCH 14
2020-08-09 02:11:42,785 Epoch  14: total training loss 19.25
2020-08-09 02:11:42,785 EPOCH 15
2020-08-09 02:11:42,835 Epoch  15: total training loss 18.56
2020-08-09 02:11:42,835 EPOCH 16
2020-08-09 02:11:42,884 Epoch  16: total training loss 17.97
2020-08-09 02:11:42,884 EPOCH 17
2020-08-09 02:11:42,935 Epoch  17: total training loss 16.48
2020-08-09 02:11:42,935 EPOCH 18
2020-08-09 02:11:42,985 Epoch  18: total training loss 19.90
2020-08-09 02:11:42,985 EPOCH 19
2020-08-09 02:11:43,034 Epoch  19: total training loss 16.20
2020-08-09 02:11:43,034 EPOCH 20
2020-08-09 02:11:43,084 Epoch  20 Step:       20 Batch Loss:    17.099443 Tokens per Sec:       82, Lr: 0.000033
2020-08-09 02:11:43,084 Epoch  20: total training loss 17.10
2020-08-09 02:11:43,084 EPOCH 21
2020-08-09 02:11:43,132 Epoch  21: total training loss 16.67
2020-08-09 02:11:43,132 EPOCH 22
2020-08-09 02:11:43,181 Epoch  22: total training loss 17.86
2020-08-09 02:11:43,181 EPOCH 23
2020-08-09 02:11:43,229 Epoch  23: total training loss 15.74
2020-08-09 02:11:43,230 EPOCH 24
2020-08-09 02:11:43,279 Epoch  24: total training loss 18.52
2020-08-09 02:11:43,279 EPOCH 25
2020-08-09 02:11:43,327 Epoch  25: total training loss 15.57
2020-08-09 02:11:43,328 EPOCH 26
2020-08-09 02:11:43,377 Epoch  26: total training loss 16.84
2020-08-09 02:11:43,377 EPOCH 27
2020-08-09 02:11:43,430 Epoch  27: total training loss 15.76
2020-08-09 02:11:43,430 EPOCH 28
2020-08-09 02:11:43,483 Epoch  28: total training loss 12.79
2020-08-09 02:11:43,484 EPOCH 29
2020-08-09 02:11:43,535 Epoch  29: total training loss 15.19
2020-08-09 02:11:43,535 EPOCH 30
2020-08-09 02:11:43,588 Epoch  30 Step:       30 Batch Loss:    13.848711 Tokens per Sec:       76, Lr: 0.000049
2020-08-09 02:11:43,589 Epoch  30: total training loss 13.85
2020-08-09 02:11:43,589 EPOCH 31
2020-08-09 02:11:43,641 Epoch  31: total training loss 15.75
2020-08-09 02:11:43,641 EPOCH 32
2020-08-09 02:11:43,692 Epoch  32: total training loss 14.91
2020-08-09 02:11:43,692 EPOCH 33
2020-08-09 02:11:43,745 Epoch  33: total training loss 15.15
2020-08-09 02:11:43,745 EPOCH 34
2020-08-09 02:11:43,793 Epoch  34: total training loss 13.07
2020-08-09 02:11:43,793 EPOCH 35
2020-08-09 02:11:43,841 Epoch  35: total training loss 16.43
2020-08-09 02:11:43,842 EPOCH 36
2020-08-09 02:11:43,890 Epoch  36: total training loss 14.19
2020-08-09 02:11:43,890 EPOCH 37
2020-08-09 02:11:43,936 Epoch  37: total training loss 12.61
2020-08-09 02:11:43,936 EPOCH 38
2020-08-09 02:11:43,983 Epoch  38: total training loss 13.50
2020-08-09 02:11:43,983 EPOCH 39
2020-08-09 02:11:44,033 Epoch  39: total training loss 11.33
2020-08-09 02:11:44,033 EPOCH 40
2020-08-09 02:11:44,081 Epoch  40 Step:       40 Batch Loss:    12.738801 Tokens per Sec:       85, Lr: 0.000065
2020-08-09 02:11:44,081 Epoch  40: total training loss 12.74
2020-08-09 02:11:44,081 EPOCH 41
2020-08-09 02:11:44,128 Epoch  41: total training loss 11.28
2020-08-09 02:11:44,128 EPOCH 42
2020-08-09 02:11:44,181 Epoch  42: total training loss 13.68
2020-08-09 02:11:44,182 EPOCH 43
2020-08-09 02:11:44,237 Epoch  43: total training loss 14.16
2020-08-09 02:11:44,237 EPOCH 44
2020-08-09 02:11:44,290 Epoch  44: total training loss 11.21
2020-08-09 02:11:44,290 EPOCH 45
2020-08-09 02:11:44,341 Epoch  45: total training loss 13.18
2020-08-09 02:11:44,341 EPOCH 46
2020-08-09 02:11:44,390 Epoch  46: total training loss 11.94
2020-08-09 02:11:44,390 EPOCH 47
2020-08-09 02:11:44,437 Epoch  47: total training loss 12.13
2020-08-09 02:11:44,437 EPOCH 48
2020-08-09 02:11:44,486 Epoch  48: total training loss 10.94
2020-08-09 02:11:44,487 EPOCH 49
2020-08-09 02:11:44,537 Epoch  49: total training loss 10.46
2020-08-09 02:11:44,537 EPOCH 50
2020-08-09 02:11:44,593 Epoch  50 Step:       50 Batch Loss:    10.581069 Tokens per Sec:       73, Lr: 0.000082
2020-08-09 02:11:44,593 Epoch  50: total training loss 10.58
2020-08-09 02:11:44,593 EPOCH 51
2020-08-09 02:11:44,647 Epoch  51: total training loss 11.06
2020-08-09 02:11:44,647 EPOCH 52
2020-08-09 02:11:44,697 Epoch  52: total training loss 10.89
2020-08-09 02:11:44,697 EPOCH 53
2020-08-09 02:11:44,746 Epoch  53: total training loss 11.90
2020-08-09 02:11:44,746 EPOCH 54
2020-08-09 02:11:44,794 Epoch  54: total training loss 11.93
2020-08-09 02:11:44,795 EPOCH 55
2020-08-09 02:11:44,841 Epoch  55: total training loss 9.49
2020-08-09 02:11:44,841 EPOCH 56
2020-08-09 02:11:44,889 Epoch  56: total training loss 9.22
2020-08-09 02:11:44,889 EPOCH 57
2020-08-09 02:11:44,938 Epoch  57: total training loss 10.85
2020-08-09 02:11:44,938 EPOCH 58
2020-08-09 02:11:44,985 Epoch  58: total training loss 9.87
2020-08-09 02:11:44,985 EPOCH 59
2020-08-09 02:11:45,033 Epoch  59: total training loss 10.42
2020-08-09 02:11:45,033 EPOCH 60
2020-08-09 02:11:45,083 Epoch  60 Step:       60 Batch Loss:     9.680832 Tokens per Sec:       81, Lr: 0.000098
2020-08-09 02:11:45,083 Epoch  60: total training loss 9.68
2020-08-09 02:11:45,083 EPOCH 61
2020-08-09 02:11:45,135 Epoch  61: total training loss 8.79
2020-08-09 02:11:45,136 EPOCH 62
2020-08-09 02:11:45,191 Epoch  62: total training loss 9.23
2020-08-09 02:11:45,191 EPOCH 63
2020-08-09 02:11:45,244 Epoch  63: total training loss 10.02
2020-08-09 02:11:45,244 EPOCH 64
2020-08-09 02:11:45,292 Epoch  64: total training loss 8.55
2020-08-09 02:11:45,292 EPOCH 65
2020-08-09 02:11:45,338 Epoch  65: total training loss 9.08
2020-08-09 02:11:45,339 EPOCH 66
2020-08-09 02:11:45,386 Epoch  66: total training loss 6.95
2020-08-09 02:11:45,386 EPOCH 67
2020-08-09 02:11:45,436 Epoch  67: total training loss 8.15
2020-08-09 02:11:45,436 EPOCH 68
2020-08-09 02:11:45,484 Epoch  68: total training loss 7.38
2020-08-09 02:11:45,484 EPOCH 69
2020-08-09 02:11:45,531 Epoch  69: total training loss 7.26
2020-08-09 02:11:45,532 EPOCH 70
2020-08-09 02:11:45,581 Epoch  70 Step:       70 Batch Loss:     6.588608 Tokens per Sec:       82, Lr: 0.000114
2020-08-09 02:11:45,581 Epoch  70: total training loss 6.59
2020-08-09 02:11:45,581 EPOCH 71
2020-08-09 02:11:45,628 Epoch  71: total training loss 7.35
2020-08-09 02:11:45,628 EPOCH 72
2020-08-09 02:11:45,676 Epoch  72: total training loss 7.35
2020-08-09 02:11:45,676 EPOCH 73
2020-08-09 02:11:45,724 Epoch  73: total training loss 6.26
2020-08-09 02:11:45,725 EPOCH 74
2020-08-09 02:11:45,772 Epoch  74: total training loss 6.29
2020-08-09 02:11:45,772 EPOCH 75
2020-08-09 02:11:45,819 Epoch  75: total training loss 6.54
2020-08-09 02:11:45,819 EPOCH 76
2020-08-09 02:11:45,868 Epoch  76: total training loss 6.94
2020-08-09 02:11:45,869 EPOCH 77
2020-08-09 02:11:45,916 Epoch  77: total training loss 6.31
2020-08-09 02:11:45,916 EPOCH 78
2020-08-09 02:11:45,966 Epoch  78: total training loss 6.39
2020-08-09 02:11:45,966 EPOCH 79
2020-08-09 02:11:46,014 Epoch  79: total training loss 6.14
2020-08-09 02:11:46,014 EPOCH 80
2020-08-09 02:11:46,063 Epoch  80 Step:       80 Batch Loss:     5.737759 Tokens per Sec:       83, Lr: 0.000130
2020-08-09 02:11:46,063 Epoch  80: total training loss 5.74
2020-08-09 02:11:46,063 EPOCH 81
2020-08-09 02:11:46,110 Epoch  81: total training loss 5.72
2020-08-09 02:11:46,111 EPOCH 82
2020-08-09 02:11:46,160 Epoch  82: total training loss 5.60
2020-08-09 02:11:46,160 EPOCH 83
2020-08-09 02:11:46,211 Epoch  83: total training loss 4.63
2020-08-09 02:11:46,212 EPOCH 84
2020-08-09 02:11:46,259 Epoch  84: total training loss 5.08
2020-08-09 02:11:46,260 EPOCH 85
2020-08-09 02:11:46,308 Epoch  85: total training loss 4.54
2020-08-09 02:11:46,309 EPOCH 86
2020-08-09 02:11:46,355 Epoch  86: total training loss 6.01
2020-08-09 02:11:46,355 EPOCH 87
2020-08-09 02:11:46,404 Epoch  87: total training loss 5.24
2020-08-09 02:11:46,404 EPOCH 88
2020-08-09 02:11:46,453 Epoch  88: total training loss 6.01
2020-08-09 02:11:46,453 EPOCH 89
2020-08-09 02:11:46,501 Epoch  89: total training loss 4.74
2020-08-09 02:11:46,501 EPOCH 90
2020-08-09 02:11:46,550 Epoch  90 Step:       90 Batch Loss:     3.776601 Tokens per Sec:       82, Lr: 0.000147
2020-08-09 02:11:46,551 Epoch  90: total training loss 3.78
2020-08-09 02:11:46,551 EPOCH 91
2020-08-09 02:11:46,599 Epoch  91: total training loss 6.80
2020-08-09 02:11:46,599 EPOCH 92
2020-08-09 02:11:46,648 Epoch  92: total training loss 4.91
2020-08-09 02:11:46,648 EPOCH 93
2020-08-09 02:11:46,695 Epoch  93: total training loss 3.59
2020-08-09 02:11:46,695 EPOCH 94
2020-08-09 02:11:46,743 Epoch  94: total training loss 3.90
2020-08-09 02:11:46,743 EPOCH 95
2020-08-09 02:11:46,791 Epoch  95: total training loss 3.68
2020-08-09 02:11:46,791 EPOCH 96
2020-08-09 02:11:46,838 Epoch  96: total training loss 3.60
2020-08-09 02:11:46,838 EPOCH 97
2020-08-09 02:11:46,887 Epoch  97: total training loss 2.94
2020-08-09 02:11:46,887 EPOCH 98
2020-08-09 02:11:46,937 Epoch  98: total training loss 2.99
2020-08-09 02:11:46,938 EPOCH 99
2020-08-09 02:11:46,986 Epoch  99: total training loss 3.94
2020-08-09 02:11:46,987 EPOCH 100
2020-08-09 02:11:47,035 Epoch 100 Step:      100 Batch Loss:     3.491515 Tokens per Sec:       83, Lr: 0.000163
2020-08-09 02:11:47,076 Hooray! New best validation result [loss]!
2020-08-09 02:11:47,077 Saving new checkpoint.
2020-08-09 02:11:47,096 Example #0
2020-08-09 02:11:47,097 	Raw hypothesis: ['alligator', 'above', 'bed']
2020-08-09 02:11:47,097 	Reference:  alligator above bed
2020-08-09 02:11:47,097 	Hypothesis: alligator above bed
2020-08-09 02:11:47,097 Validation result (greedy) at epoch 100, step      100: sequence_accuracy: 100.00, loss:   2.0937, ppl:   1.6878, duration: 0.0615s
2020-08-09 02:11:47,097 Epoch 100: total training loss 3.49
2020-08-09 02:11:47,097 Training ended after 100 epochs.
2020-08-09 02:11:47,097 Best validation result (greedy) at step      100:   2.09 loss.
2020-08-09 02:11:47,206 test sequence_accuracy: 100.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-08-09 02:11:47,206 Translations saved to: models/transformer/00000100.hyps.test
