2020-08-09 03:07:26,129 Hello! This is Joey-NMT.
2020-08-09 03:07:40,148 Total params: 472460
2020-08-09 03:07:40,151 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'trg_embed.lut.weight']
2020-08-09 03:07:40,155 cfg.name                           : my_experiment
2020-08-09 03:07:40,156 cfg.data.src                       : data
2020-08-09 03:07:40,156 cfg.data.trg                       : en
2020-08-09 03:07:40,156 cfg.data.train                     : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/data/lists/train
2020-08-09 03:07:40,156 cfg.data.dev                       : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/data/lists/dev
2020-08-09 03:07:40,156 cfg.data.test                      : /home/dhruva/Desktop/CopyCat/Deep-Learning-for-ASLR/data/lists/test
2020-08-09 03:07:40,157 cfg.data.level                     : word
2020-08-09 03:07:40,157 cfg.data.lowercase                 : True
2020-08-09 03:07:40,157 cfg.data.max_src_length            : 400
2020-08-09 03:07:40,157 cfg.data.max_trg_length            : 7
2020-08-09 03:07:40,157 cfg.data.src_voc_min_freq          : 1
2020-08-09 03:07:40,158 cfg.data.src_voc_limit             : 101
2020-08-09 03:07:40,158 cfg.data.trg_voc_min_freq          : 1
2020-08-09 03:07:40,158 cfg.data.trg_voc_limit             : -1
2020-08-09 03:07:40,158 cfg.testing.beam_size              : 5
2020-08-09 03:07:40,158 cfg.testing.alpha                  : 1.0
2020-08-09 03:07:40,159 cfg.training.reset_best_ckpt       : False
2020-08-09 03:07:40,159 cfg.training.reset_scheduler       : False
2020-08-09 03:07:40,159 cfg.training.reset_optimizer       : False
2020-08-09 03:07:40,159 cfg.training.random_seed           : 42
2020-08-09 03:07:40,159 cfg.training.optimizer             : adam
2020-08-09 03:07:40,159 cfg.training.adam_betas            : [0.9, 0.98]
2020-08-09 03:07:40,160 cfg.training.learning_rate         : 0.001
2020-08-09 03:07:40,160 cfg.training.learning_rate_min     : 1e-06
2020-08-09 03:07:40,160 cfg.training.learning_rate_factor  : 0.5
2020-08-09 03:07:40,160 cfg.training.learning_rate_warmup  : 1000
2020-08-09 03:07:40,160 cfg.training.weight_decay          : 0.0
2020-08-09 03:07:40,161 cfg.training.batch_size            : 64
2020-08-09 03:07:40,161 cfg.training.batch_type            : token
2020-08-09 03:07:40,161 cfg.training.batch_multiplier      : 1
2020-08-09 03:07:40,161 cfg.training.normalization         : batch
2020-08-09 03:07:40,161 cfg.training.scheduling            : noam
2020-08-09 03:07:40,161 cfg.training.epochs                : 100
2020-08-09 03:07:40,162 cfg.training.validation_freq       : 100
2020-08-09 03:07:40,162 cfg.training.logging_freq          : 10
2020-08-09 03:07:40,162 cfg.training.eval_metric           : sequence_accuracy
2020-08-09 03:07:40,162 cfg.training.early_stopping_metric : loss
2020-08-09 03:07:40,162 cfg.training.model_dir             : models/transformer
2020-08-09 03:07:40,162 cfg.training.overwrite             : True
2020-08-09 03:07:40,163 cfg.training.shuffle               : True
2020-08-09 03:07:40,163 cfg.training.use_cuda              : False
2020-08-09 03:07:40,163 cfg.training.max_output_length     : 31
2020-08-09 03:07:40,163 cfg.training.print_valid_sents     : [0, 1, 2]
2020-08-09 03:07:40,163 cfg.training.keep_last_ckpts       : 3
2020-08-09 03:07:40,164 cfg.training.label_smoothing       : 0.0
2020-08-09 03:07:40,164 cfg.model.initializer              : xavier
2020-08-09 03:07:40,164 cfg.model.init_gain                : 1.0
2020-08-09 03:07:40,164 cfg.model.bias_initializer         : zeros
2020-08-09 03:07:40,164 cfg.model.embed_initializer        : xavier
2020-08-09 03:07:40,165 cfg.model.embed_init_gain          : 1.0
2020-08-09 03:07:40,165 cfg.model.tied_embeddings          : False
2020-08-09 03:07:40,165 cfg.model.tied_softmax             : True
2020-08-09 03:07:40,165 cfg.model.encoder.type             : transformer
2020-08-09 03:07:40,165 cfg.model.encoder.num_layers       : 3
2020-08-09 03:07:40,165 cfg.model.encoder.num_heads        : 2
2020-08-09 03:07:40,166 cfg.model.encoder.embeddings.embedding_dim : 94
2020-08-09 03:07:40,166 cfg.model.encoder.embeddings.scale : True
2020-08-09 03:07:40,166 cfg.model.encoder.embeddings.freeze : False
2020-08-09 03:07:40,166 cfg.model.encoder.hidden_size      : 94
2020-08-09 03:07:40,167 cfg.model.encoder.ff_size          : 128
2020-08-09 03:07:40,167 cfg.model.encoder.dropout          : 0.1
2020-08-09 03:07:40,167 cfg.model.encoder.freeze           : False
2020-08-09 03:07:40,168 cfg.model.decoder.type             : transformer
2020-08-09 03:07:40,168 cfg.model.decoder.num_layers       : 3
2020-08-09 03:07:40,168 cfg.model.decoder.num_heads        : 2
2020-08-09 03:07:40,168 cfg.model.decoder.embeddings.embedding_dim : 94
2020-08-09 03:07:40,169 cfg.model.decoder.embeddings.scale : True
2020-08-09 03:07:40,169 cfg.model.decoder.embeddings.freeze : False
2020-08-09 03:07:40,169 cfg.model.decoder.hidden_size      : 94
2020-08-09 03:07:40,169 cfg.model.decoder.ff_size          : 128
2020-08-09 03:07:40,170 cfg.model.decoder.dropout          : 0.1
2020-08-09 03:07:40,170 cfg.model.decoder.freeze           : False
2020-08-09 03:07:40,170 Model(
	encoder=TransformerEncoder(num_layers=3, num_heads=2),
	decoder=TransformerDecoder(num_layers=3, num_heads=2),
	trg_embed=Embeddings(embedding_dim=94, vocab_size=22))
2020-08-09 03:07:40,814 EPOCH 1
2020-08-09 03:07:50,682 Epoch   1: total training loss 116.23
2020-08-09 03:07:50,682 EPOCH 2
2020-08-09 03:07:59,766 Epoch   2 Step:       10 Batch Loss:    21.570179 Tokens per Sec:      157, Lr: 0.000016
2020-08-09 03:07:59,766 Epoch   2: total training loss 108.01
2020-08-09 03:07:59,766 EPOCH 3
2020-08-09 03:08:08,918 Epoch   3: total training loss 106.33
2020-08-09 03:08:08,919 EPOCH 4
2020-08-09 03:08:17,632 Epoch   4 Step:       20 Batch Loss:    20.962421 Tokens per Sec:      164, Lr: 0.000033
2020-08-09 03:08:17,632 Epoch   4: total training loss 104.35
2020-08-09 03:08:17,632 EPOCH 5
2020-08-09 03:08:26,759 Epoch   5: total training loss 101.57
2020-08-09 03:08:26,759 EPOCH 6
2020-08-09 03:08:35,753 Epoch   6 Step:       30 Batch Loss:    19.598787 Tokens per Sec:      159, Lr: 0.000049
2020-08-09 03:08:35,753 Epoch   6: total training loss 98.84
2020-08-09 03:08:35,753 EPOCH 7
2020-08-09 03:08:44,431 Epoch   7: total training loss 94.62
2020-08-09 03:08:44,431 EPOCH 8
2020-08-09 03:08:53,367 Epoch   8 Step:       40 Batch Loss:    17.767540 Tokens per Sec:      160, Lr: 0.000065
2020-08-09 03:08:53,367 Epoch   8: total training loss 90.71
2020-08-09 03:08:53,368 EPOCH 9
2020-08-09 03:09:01,940 Epoch   9: total training loss 87.56
2020-08-09 03:09:01,941 EPOCH 10
2020-08-09 03:09:10,829 Epoch  10 Step:       50 Batch Loss:    16.221573 Tokens per Sec:      161, Lr: 0.000082
2020-08-09 03:09:10,830 Epoch  10: total training loss 83.71
2020-08-09 03:09:10,830 EPOCH 11
2020-08-09 03:09:19,787 Epoch  11: total training loss 80.65
2020-08-09 03:09:19,787 EPOCH 12
2020-08-09 03:09:28,642 Epoch  12 Step:       60 Batch Loss:    15.104074 Tokens per Sec:      162, Lr: 0.000098
2020-08-09 03:09:28,643 Epoch  12: total training loss 78.26
2020-08-09 03:09:28,643 EPOCH 13
2020-08-09 03:09:37,479 Epoch  13: total training loss 75.83
2020-08-09 03:09:37,479 EPOCH 14
2020-08-09 03:09:46,693 Epoch  14 Step:       70 Batch Loss:    14.297543 Tokens per Sec:      155, Lr: 0.000114
2020-08-09 03:09:46,694 Epoch  14: total training loss 73.23
2020-08-09 03:09:46,694 EPOCH 15
2020-08-09 03:09:56,721 Epoch  15: total training loss 71.74
2020-08-09 03:09:56,721 EPOCH 16
2020-08-09 03:10:07,061 Epoch  16 Step:       80 Batch Loss:    13.647497 Tokens per Sec:      138, Lr: 0.000130
2020-08-09 03:10:07,061 Epoch  16: total training loss 69.39
2020-08-09 03:10:07,061 EPOCH 17
2020-08-09 03:10:16,966 Epoch  17: total training loss 67.79
2020-08-09 03:10:16,967 EPOCH 18
2020-08-09 03:10:27,208 Epoch  18 Step:       90 Batch Loss:    13.035482 Tokens per Sec:      140, Lr: 0.000147
2020-08-09 03:10:27,208 Epoch  18: total training loss 66.24
2020-08-09 03:10:27,208 EPOCH 19
2020-08-09 03:10:37,403 Epoch  19: total training loss 64.10
2020-08-09 03:10:37,403 EPOCH 20
2020-08-09 03:10:47,492 Epoch  20 Step:      100 Batch Loss:    12.135801 Tokens per Sec:      142, Lr: 0.000163
2020-08-09 03:11:00,684 Hooray! New best validation result [loss]!
2020-08-09 03:11:00,684 Saving new checkpoint.
2020-08-09 03:11:00,724 Example #0
2020-08-09 03:11:00,724 	Raw hypothesis: ['lion']
2020-08-09 03:11:00,724 	Reference:  black lion above grey bed
2020-08-09 03:11:00,724 	Hypothesis: lion
2020-08-09 03:11:00,724 Example #1
2020-08-09 03:11:00,724 	Raw hypothesis: ['monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey', 'monkey']
2020-08-09 03:11:00,724 	Reference:  lion above box
2020-08-09 03:11:00,724 	Hypothesis: monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey monkey
2020-08-09 03:11:00,724 Example #2
2020-08-09 03:11:00,724 	Raw hypothesis: ['alligator', 'alligator', 'alligator']
2020-08-09 03:11:00,724 	Reference:  alligator above wall
2020-08-09 03:11:00,724 	Hypothesis: alligator alligator alligator
2020-08-09 03:11:00,725 Validation result (greedy) at epoch  20, step      100: sequence_accuracy:   0.00, loss: 3415.8130, ppl:  10.8991, duration: 13.2324s
2020-08-09 03:11:00,725 Epoch  20: total training loss 62.60
2020-08-09 03:11:00,725 EPOCH 21
2020-08-09 03:11:08,885 Epoch  21: total training loss 60.69
2020-08-09 03:11:08,886 EPOCH 22
2020-08-09 03:11:16,594 Epoch  22 Step:      110 Batch Loss:    11.427653 Tokens per Sec:      186, Lr: 0.000179
2020-08-09 03:11:16,595 Epoch  22: total training loss 58.71
2020-08-09 03:11:16,595 EPOCH 23
2020-08-09 03:11:24,344 Epoch  23: total training loss 56.49
2020-08-09 03:11:24,344 EPOCH 24
2020-08-09 03:11:32,295 Epoch  24 Step:      120 Batch Loss:    10.459084 Tokens per Sec:      180, Lr: 0.000196
2020-08-09 03:11:32,295 Epoch  24: total training loss 54.23
2020-08-09 03:11:32,295 EPOCH 25
2020-08-09 03:11:40,398 Epoch  25: total training loss 52.16
2020-08-09 03:11:40,398 EPOCH 26
2020-08-09 03:11:48,387 Epoch  26 Step:      130 Batch Loss:     9.730005 Tokens per Sec:      179, Lr: 0.000212
2020-08-09 03:11:48,387 Epoch  26: total training loss 49.80
2020-08-09 03:11:48,387 EPOCH 27
2020-08-09 03:11:56,484 Epoch  27: total training loss 47.43
2020-08-09 03:11:56,484 EPOCH 28
2020-08-09 03:12:04,372 Epoch  28 Step:      140 Batch Loss:     8.743196 Tokens per Sec:      181, Lr: 0.000228
2020-08-09 03:12:04,372 Epoch  28: total training loss 45.47
2020-08-09 03:12:04,372 EPOCH 29
2020-08-09 03:12:12,456 Epoch  29: total training loss 43.04
2020-08-09 03:12:12,456 EPOCH 30
2020-08-09 03:12:20,426 Epoch  30 Step:      150 Batch Loss:     8.215791 Tokens per Sec:      179, Lr: 0.000245
2020-08-09 03:12:20,427 Epoch  30: total training loss 41.54
2020-08-09 03:12:20,427 EPOCH 31
2020-08-09 03:12:28,230 Epoch  31: total training loss 38.90
2020-08-09 03:12:28,230 EPOCH 32
2020-08-09 03:12:36,363 Epoch  32 Step:      160 Batch Loss:     7.435688 Tokens per Sec:      176, Lr: 0.000261
2020-08-09 03:12:36,363 Epoch  32: total training loss 37.74
2020-08-09 03:12:36,363 EPOCH 33
2020-08-09 03:12:44,185 Epoch  33: total training loss 35.34
2020-08-09 03:12:44,185 EPOCH 34
2020-08-09 03:12:51,989 Epoch  34 Step:      170 Batch Loss:     6.587487 Tokens per Sec:      183, Lr: 0.000277
2020-08-09 03:12:51,989 Epoch  34: total training loss 34.04
2020-08-09 03:12:51,989 EPOCH 35
2020-08-09 03:13:00,005 Epoch  35: total training loss 33.18
2020-08-09 03:13:00,005 EPOCH 36
2020-08-09 03:13:07,676 Epoch  36 Step:      180 Batch Loss:     6.163363 Tokens per Sec:      186, Lr: 0.000294
2020-08-09 03:13:07,676 Epoch  36: total training loss 32.50
2020-08-09 03:13:07,676 EPOCH 37
2020-08-09 03:13:15,626 Epoch  37: total training loss 30.59
2020-08-09 03:13:15,626 EPOCH 38
2020-08-09 03:13:23,529 Epoch  38 Step:      190 Batch Loss:     5.677207 Tokens per Sec:      181, Lr: 0.000310
2020-08-09 03:13:23,529 Epoch  38: total training loss 29.86
2020-08-09 03:13:23,529 EPOCH 39
2020-08-09 03:13:31,332 Epoch  39: total training loss 29.16
2020-08-09 03:13:31,332 EPOCH 40
2020-08-09 03:13:39,338 Epoch  40 Step:      200 Batch Loss:     5.209570 Tokens per Sec:      179, Lr: 0.000326
2020-08-09 03:13:45,131 Hooray! New best validation result [loss]!
2020-08-09 03:13:45,131 Saving new checkpoint.
2020-08-09 03:13:45,157 Example #0
2020-08-09 03:13:45,157 	Raw hypothesis: ['lion', 'above', 'blue', 'wall']
2020-08-09 03:13:45,158 	Reference:  black lion above grey bed
2020-08-09 03:13:45,158 	Hypothesis: lion above blue wall
2020-08-09 03:13:45,158 Example #1
2020-08-09 03:13:45,158 	Raw hypothesis: ['lion', 'above', 'blue', 'wall']
2020-08-09 03:13:45,158 	Reference:  lion above box
2020-08-09 03:13:45,158 	Hypothesis: lion above blue wall
2020-08-09 03:13:45,158 Example #2
2020-08-09 03:13:45,158 	Raw hypothesis: ['monkey', 'above', 'wall']
2020-08-09 03:13:45,158 	Reference:  alligator above wall
2020-08-09 03:13:45,158 	Hypothesis: monkey above wall
2020-08-09 03:13:45,158 Validation result (greedy) at epoch  40, step      200: sequence_accuracy:   7.29, loss: 1519.7190, ppl:   2.8943, duration: 5.8198s
2020-08-09 03:13:45,159 Epoch  40: total training loss 28.59
2020-08-09 03:13:45,159 EPOCH 41
2020-08-09 03:13:53,086 Epoch  41: total training loss 27.76
2020-08-09 03:13:53,086 EPOCH 42
2020-08-09 03:14:00,985 Epoch  42 Step:      210 Batch Loss:     5.004909 Tokens per Sec:      181, Lr: 0.000342
2020-08-09 03:14:00,986 Epoch  42: total training loss 26.88
2020-08-09 03:14:00,986 EPOCH 43
2020-08-09 03:14:08,862 Epoch  43: total training loss 26.54
2020-08-09 03:14:08,862 EPOCH 44
2020-08-09 03:14:16,839 Epoch  44 Step:      220 Batch Loss:     5.029135 Tokens per Sec:      179, Lr: 0.000359
2020-08-09 03:14:16,839 Epoch  44: total training loss 26.08
2020-08-09 03:14:16,839 EPOCH 45
2020-08-09 03:14:24,879 Epoch  45: total training loss 25.39
2020-08-09 03:14:24,880 EPOCH 46
2020-08-09 03:14:32,704 Epoch  46 Step:      230 Batch Loss:     4.823981 Tokens per Sec:      183, Lr: 0.000375
2020-08-09 03:14:32,704 Epoch  46: total training loss 24.66
2020-08-09 03:14:32,704 EPOCH 47
2020-08-09 03:14:40,601 Epoch  47: total training loss 24.42
2020-08-09 03:14:40,601 EPOCH 48
2020-08-09 03:14:48,405 Epoch  48 Step:      240 Batch Loss:     4.452324 Tokens per Sec:      183, Lr: 0.000391
2020-08-09 03:14:48,405 Epoch  48: total training loss 23.88
2020-08-09 03:14:48,405 EPOCH 49
2020-08-09 03:14:56,201 Epoch  49: total training loss 23.38
2020-08-09 03:14:56,201 EPOCH 50
2020-08-09 03:15:04,326 Epoch  50 Step:      250 Batch Loss:     4.432104 Tokens per Sec:      176, Lr: 0.000408
2020-08-09 03:15:04,327 Epoch  50: total training loss 22.89
2020-08-09 03:15:04,327 EPOCH 51
2020-08-09 03:15:12,145 Epoch  51: total training loss 22.89
2020-08-09 03:15:12,145 EPOCH 52
2020-08-09 03:15:20,110 Epoch  52 Step:      260 Batch Loss:     4.168969 Tokens per Sec:      180, Lr: 0.000424
2020-08-09 03:15:20,111 Epoch  52: total training loss 22.13
2020-08-09 03:15:20,111 EPOCH 53
2020-08-09 03:15:27,978 Epoch  53: total training loss 22.06
2020-08-09 03:15:27,979 EPOCH 54
2020-08-09 03:15:35,787 Epoch  54 Step:      270 Batch Loss:     3.825932 Tokens per Sec:      183, Lr: 0.000440
2020-08-09 03:15:35,787 Epoch  54: total training loss 21.28
2020-08-09 03:15:35,787 EPOCH 55
2020-08-09 03:15:43,791 Epoch  55: total training loss 20.98
2020-08-09 03:15:43,792 EPOCH 56
2020-08-09 03:15:51,781 Epoch  56 Step:      280 Batch Loss:     3.890178 Tokens per Sec:      179, Lr: 0.000457
2020-08-09 03:15:51,781 Epoch  56: total training loss 20.77
2020-08-09 03:15:51,781 EPOCH 57
2020-08-09 03:15:59,636 Epoch  57: total training loss 20.76
2020-08-09 03:15:59,636 EPOCH 58
2020-08-09 03:16:07,675 Epoch  58 Step:      290 Batch Loss:     3.784818 Tokens per Sec:      178, Lr: 0.000473
2020-08-09 03:16:07,675 Epoch  58: total training loss 20.28
2020-08-09 03:16:07,675 EPOCH 59
2020-08-09 03:16:15,647 Epoch  59: total training loss 20.39
2020-08-09 03:16:15,647 EPOCH 60
2020-08-09 03:16:23,431 Epoch  60 Step:      300 Batch Loss:     3.474143 Tokens per Sec:      184, Lr: 0.000489
2020-08-09 03:16:29,615 Hooray! New best validation result [loss]!
2020-08-09 03:16:29,615 Saving new checkpoint.
2020-08-09 03:16:29,640 Example #0
2020-08-09 03:16:29,640 	Raw hypothesis: ['lion', 'above', 'wall']
2020-08-09 03:16:29,640 	Reference:  black lion above grey bed
2020-08-09 03:16:29,641 	Hypothesis: lion above wall
2020-08-09 03:16:29,641 Example #1
2020-08-09 03:16:29,641 	Raw hypothesis: ['lion', 'above', 'wall']
2020-08-09 03:16:29,641 	Reference:  lion above box
2020-08-09 03:16:29,641 	Hypothesis: lion above wall
2020-08-09 03:16:29,641 Example #2
2020-08-09 03:16:29,641 	Raw hypothesis: ['alligator', 'above', 'wall']
2020-08-09 03:16:29,641 	Reference:  alligator above wall
2020-08-09 03:16:29,641 	Hypothesis: alligator above wall
2020-08-09 03:16:29,641 Validation result (greedy) at epoch  60, step      300: sequence_accuracy:  18.75, loss: 1132.7606, ppl:   2.2081, duration: 6.2097s
2020-08-09 03:16:29,642 Epoch  60: total training loss 19.24
2020-08-09 03:16:29,642 EPOCH 61
2020-08-09 03:16:37,622 Epoch  61: total training loss 19.13
2020-08-09 03:16:37,622 EPOCH 62
2020-08-09 03:16:45,323 Epoch  62 Step:      310 Batch Loss:     3.458882 Tokens per Sec:      186, Lr: 0.000506
2020-08-09 03:16:45,323 Epoch  62: total training loss 19.27
2020-08-09 03:16:45,323 EPOCH 63
2020-08-09 03:16:53,308 Epoch  63: total training loss 18.50
2020-08-09 03:16:53,308 EPOCH 64
2020-08-09 03:17:01,254 Epoch  64 Step:      320 Batch Loss:     3.079812 Tokens per Sec:      180, Lr: 0.000522
2020-08-09 03:17:01,254 Epoch  64: total training loss 17.67
2020-08-09 03:17:01,254 EPOCH 65
2020-08-09 03:17:08,850 Epoch  65: total training loss 17.05
2020-08-09 03:17:08,850 EPOCH 66
2020-08-09 03:17:16,685 Epoch  66 Step:      330 Batch Loss:     3.140742 Tokens per Sec:      183, Lr: 0.000538
2020-08-09 03:17:16,685 Epoch  66: total training loss 17.44
2020-08-09 03:17:16,686 EPOCH 67
2020-08-09 03:17:24,393 Epoch  67: total training loss 17.13
2020-08-09 03:17:24,393 EPOCH 68
2020-08-09 03:17:32,366 Epoch  68 Step:      340 Batch Loss:     3.318008 Tokens per Sec:      179, Lr: 0.000554
2020-08-09 03:17:32,366 Epoch  68: total training loss 16.98
2020-08-09 03:17:32,366 EPOCH 69
2020-08-09 03:17:40,559 Epoch  69: total training loss 16.55
2020-08-09 03:17:40,559 EPOCH 70
2020-08-09 03:17:48,323 Epoch  70 Step:      350 Batch Loss:     3.016811 Tokens per Sec:      184, Lr: 0.000571
2020-08-09 03:17:48,323 Epoch  70: total training loss 16.73
2020-08-09 03:17:48,323 EPOCH 71
2020-08-09 03:17:56,284 Epoch  71: total training loss 15.90
2020-08-09 03:17:56,285 EPOCH 72
2020-08-09 03:18:04,266 Epoch  72 Step:      360 Batch Loss:     2.648938 Tokens per Sec:      179, Lr: 0.000587
2020-08-09 03:18:04,266 Epoch  72: total training loss 15.58
2020-08-09 03:18:04,266 EPOCH 73
2020-08-09 03:18:11,911 Epoch  73: total training loss 15.96
2020-08-09 03:18:11,911 EPOCH 74
2020-08-09 03:18:19,838 Epoch  74 Step:      370 Batch Loss:     2.892509 Tokens per Sec:      180, Lr: 0.000603
2020-08-09 03:18:19,839 Epoch  74: total training loss 15.18
2020-08-09 03:18:19,839 EPOCH 75
2020-08-09 03:18:27,801 Epoch  75: total training loss 15.06
2020-08-09 03:18:27,801 EPOCH 76
2020-08-09 03:18:35,332 Epoch  76 Step:      380 Batch Loss:     2.380671 Tokens per Sec:      190, Lr: 0.000620
2020-08-09 03:18:35,332 Epoch  76: total training loss 14.38
2020-08-09 03:18:35,333 EPOCH 77
2020-08-09 03:18:43,344 Epoch  77: total training loss 14.43
2020-08-09 03:18:43,344 EPOCH 78
2020-08-09 03:18:51,008 Epoch  78 Step:      390 Batch Loss:     2.369531 Tokens per Sec:      187, Lr: 0.000636
2020-08-09 03:18:51,009 Epoch  78: total training loss 13.95
2020-08-09 03:18:51,009 EPOCH 79
2020-08-09 03:18:59,064 Epoch  79: total training loss 14.03
2020-08-09 03:18:59,064 EPOCH 80
2020-08-09 03:19:07,087 Epoch  80 Step:      400 Batch Loss:     2.280690 Tokens per Sec:      178, Lr: 0.000652
2020-08-09 03:19:13,438 Hooray! New best validation result [loss]!
2020-08-09 03:19:13,438 Saving new checkpoint.
2020-08-09 03:19:13,464 Example #0
2020-08-09 03:19:13,464 	Raw hypothesis: ['lion', 'above', 'wall']
2020-08-09 03:19:13,464 	Reference:  black lion above grey bed
2020-08-09 03:19:13,464 	Hypothesis: lion above wall
2020-08-09 03:19:13,464 Example #1
2020-08-09 03:19:13,465 	Raw hypothesis: ['lion', 'above', 'box']
2020-08-09 03:19:13,465 	Reference:  lion above box
2020-08-09 03:19:13,465 	Hypothesis: lion above box
2020-08-09 03:19:13,465 Example #2
2020-08-09 03:19:13,465 	Raw hypothesis: ['alligator', 'in', 'box']
2020-08-09 03:19:13,465 	Reference:  alligator above wall
2020-08-09 03:19:13,465 	Hypothesis: alligator in box
2020-08-09 03:19:13,465 Validation result (greedy) at epoch  80, step      400: sequence_accuracy:  27.43, loss: 778.3701, ppl:   1.7234, duration: 6.3777s
2020-08-09 03:19:13,465 Epoch  80: total training loss 13.57
2020-08-09 03:19:13,466 EPOCH 81
2020-08-09 03:19:21,248 Epoch  81: total training loss 13.29
2020-08-09 03:19:21,248 EPOCH 82
2020-08-09 03:19:29,009 Epoch  82 Step:      410 Batch Loss:     2.348171 Tokens per Sec:      184, Lr: 0.000669
2020-08-09 03:19:29,009 Epoch  82: total training loss 13.26
2020-08-09 03:19:29,009 EPOCH 83
2020-08-09 03:19:36,593 Epoch  83: total training loss 13.22
2020-08-09 03:19:36,593 EPOCH 84
2020-08-09 03:19:44,474 Epoch  84 Step:      420 Batch Loss:     2.544122 Tokens per Sec:      181, Lr: 0.000685
2020-08-09 03:19:44,474 Epoch  84: total training loss 13.24
2020-08-09 03:19:44,474 EPOCH 85
2020-08-09 03:19:52,332 Epoch  85: total training loss 12.60
2020-08-09 03:19:52,332 EPOCH 86
2020-08-09 03:19:59,882 Epoch  86 Step:      430 Batch Loss:     2.273616 Tokens per Sec:      189, Lr: 0.000701
2020-08-09 03:19:59,882 Epoch  86: total training loss 12.84
2020-08-09 03:19:59,882 EPOCH 87
2020-08-09 03:20:07,811 Epoch  87: total training loss 12.34
2020-08-09 03:20:07,812 EPOCH 88
2020-08-09 03:20:15,855 Epoch  88 Step:      440 Batch Loss:     2.001063 Tokens per Sec:      178, Lr: 0.000718
2020-08-09 03:20:15,855 Epoch  88: total training loss 12.22
2020-08-09 03:20:15,855 EPOCH 89
2020-08-09 03:20:23,758 Epoch  89: total training loss 11.73
2020-08-09 03:20:23,758 EPOCH 90
2020-08-09 03:20:31,853 Epoch  90 Step:      450 Batch Loss:     2.122783 Tokens per Sec:      177, Lr: 0.000734
2020-08-09 03:20:31,853 Epoch  90: total training loss 11.93
2020-08-09 03:20:31,853 EPOCH 91
2020-08-09 03:20:39,906 Epoch  91: total training loss 11.89
2020-08-09 03:20:39,906 EPOCH 92
2020-08-09 03:20:47,686 Epoch  92 Step:      460 Batch Loss:     2.137262 Tokens per Sec:      184, Lr: 0.000750
2020-08-09 03:20:47,686 Epoch  92: total training loss 12.15
2020-08-09 03:20:47,686 EPOCH 93
2020-08-09 03:20:55,739 Epoch  93: total training loss 10.98
2020-08-09 03:20:55,739 EPOCH 94
2020-08-09 03:21:03,845 Epoch  94 Step:      470 Batch Loss:     1.865652 Tokens per Sec:      176, Lr: 0.000766
2020-08-09 03:21:03,846 Epoch  94: total training loss 11.84
2020-08-09 03:21:03,846 EPOCH 95
2020-08-09 03:21:11,651 Epoch  95: total training loss 10.94
2020-08-09 03:21:11,651 EPOCH 96
2020-08-09 03:21:19,711 Epoch  96 Step:      480 Batch Loss:     1.886517 Tokens per Sec:      177, Lr: 0.000783
2020-08-09 03:21:19,711 Epoch  96: total training loss 11.06
2020-08-09 03:21:19,711 EPOCH 97
2020-08-09 03:21:27,778 Epoch  97: total training loss 10.65
2020-08-09 03:21:27,779 EPOCH 98
2020-08-09 03:21:35,608 Epoch  98 Step:      490 Batch Loss:     1.626328 Tokens per Sec:      183, Lr: 0.000799
2020-08-09 03:21:35,608 Epoch  98: total training loss 10.51
2020-08-09 03:21:35,608 EPOCH 99
2020-08-09 03:21:43,710 Epoch  99: total training loss 10.57
2020-08-09 03:21:43,710 EPOCH 100
2020-08-09 03:21:51,577 Epoch 100 Step:      500 Batch Loss:     1.712172 Tokens per Sec:      182, Lr: 0.000815
2020-08-09 03:22:00,065 Hooray! New best validation result [loss]!
2020-08-09 03:22:00,065 Saving new checkpoint.
2020-08-09 03:22:00,092 Example #0
2020-08-09 03:22:00,092 	Raw hypothesis: ['lion', 'above', 'orange', 'wall']
2020-08-09 03:22:00,092 	Reference:  black lion above grey bed
2020-08-09 03:22:00,092 	Hypothesis: lion above orange wall
2020-08-09 03:22:00,092 Example #1
2020-08-09 03:22:00,092 	Raw hypothesis: ['lion', 'above', 'box']
2020-08-09 03:22:00,092 	Reference:  lion above box
2020-08-09 03:22:00,092 	Hypothesis: lion above box
2020-08-09 03:22:00,092 Example #2
2020-08-09 03:22:00,092 	Raw hypothesis: ['alligator', 'above', 'wall']
2020-08-09 03:22:00,092 	Reference:  alligator above wall
2020-08-09 03:22:00,092 	Hypothesis: alligator above wall
2020-08-09 03:22:00,092 Validation result (greedy) at epoch 100, step      500: sequence_accuracy:  35.07, loss: 730.9463, ppl:   1.6672, duration: 8.5152s
2020-08-09 03:22:00,093 Epoch 100: total training loss 10.45
2020-08-09 03:22:00,093 Training ended after 100 epochs.
2020-08-09 03:22:00,093 Best validation result (greedy) at step      500: 730.95 loss.
