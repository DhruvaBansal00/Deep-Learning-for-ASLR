{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GRU_with_Attention.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"v5eNYJdhNMlG","colab_type":"text"},"source":["Basic Setup\n"]},{"cell_type":"code","metadata":{"id":"Iw6d7m5MKJfW","colab_type":"code","outputId":"c208f7bc-9fc1-4b03-88fc-ab9d92edb7a6","executionInfo":{"status":"ok","timestamp":1589389148753,"user_tz":-330,"elapsed":7706,"user":{"displayName":"DHRUVA BANSAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjozSR-DG0yT8nOlVOylFHoeIYpSXFFQsi3040ccw=s64","userId":"12303647104962841566"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["pip install torch torchvision\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nFbaOUiWKrTB","colab_type":"code","outputId":"d1855fc7-e825-4f50-f47c-772237ca1bde","executionInfo":{"status":"ok","timestamp":1589389198113,"user_tz":-330,"elapsed":57048,"user":{"displayName":"DHRUVA BANSAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjozSR-DG0yT8nOlVOylFHoeIYpSXFFQsi3040ccw=s64","userId":"12303647104962841566"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import torch\n","print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/LSTM/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/LSTM\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zekYo16sP1Qr","colab_type":"text"},"source":["Language Class"]},{"cell_type":"code","metadata":{"id":"GTDU_GvvPzf0","colab_type":"code","colab":{}},"source":["class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {}\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrsUvZv5Kjkd","colab_type":"text"},"source":["Encoder GRU setup"]},{"cell_type":"code","metadata":{"id":"MfH2NvaTKfu7","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch\n","\n","class EncoderGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout=0.1):\n","        super(EncoderGRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        output, hidden = self.gru(input, hidden)\n","        return output, hidden\n","\n","    def initHidden(self, batch=1):\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        return torch.zeros(1, batch, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLbpsGXQyz-Y","colab_type":"text"},"source":["Decoder GRU with Attention"]},{"cell_type":"code","metadata":{"id":"JyZmdYdEy1w9","colab_type":"code","colab":{}},"source":["class AttnDecoderGRU(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout=0.1, max_input_length=470):\n","        super(AttnDecoderGRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout = dropout\n","        self.max_input_length = max_input_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_input_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        # self.attention_network = nn.Linear(self.hidden_size * 2, self.output_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","        self.out_attn = nn.Linear(self.hidden_size * 2, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        # #####################version 1#######################\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        # attn_weights = self.dropout(attn_weights)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","        ####################version 2##########################\n","        # embedded = self.embedding(input).view(1, 1, -1)\n","        # _, hidden = self.gru(embedded, hidden)\n","\n","        # attn_weights = F.softmax(torch.mm(hidden[0], encoder_outputs.t()), dim=1)\n","        # attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n","        \n","        # attention_res = torch.cat((hidden[0], attn_applied[0]), 1)\n","        # output = F.log_softmax(self.attention_network(attention_res), dim=1)\n","\n","        # return output, hidden, attn_weights\n","        ####################version 3##############################\n","        # embedded = self.embedding(input).view(1, 1, -1)\n","        # _, hidden = self.gru(embedded, hidden)\n","\n","        # attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        # attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n","\n","        # attention_res = torch.cat((hidden[0], attn_applied[0]), 1)\n","\n","        # output = F.log_softmax(self.out_attn(attention_res), dim=1)\n","        # return output, hidden, attn_weights\n","        #####################version 4############################\n","        # embedded = self.embedding(input).view(1, 1, -1)\n","\n","        # attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        # attn_weights = self.dropout(attn_weights)\n","        # attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","        #                          encoder_outputs.unsqueeze(0))\n","\n","        # output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        # output = self.attn_combine(output).unsqueeze(0)\n","\n","        # output = F.relu(output)\n","        # output, hidden = self.gru(output, hidden)\n","        # output = F.log_softmax(self.out(output[0]), dim=1)\n","        # return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNmd0iM_LVcG","colab_type":"text"},"source":["Cross - Validation Fold generator"]},{"cell_type":"code","metadata":{"id":"q3eoOWNAKgAA","colab_type":"code","colab":{}},"source":["import random\n","import torch\n","from sklearn import model_selection\n","\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence, device):\n","    indexes = indexesFromSentence(lang, sentence)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def split(pairs, lang, device):\n","    train = []\n","    test = []\n","    for label in pairs:\n","        label_tensor = tensorFromSentence(lang, label, device)\n","        iters = pairs[label]\n","        test_index = random.randint(0, len(iters) - 1)\n","        accept_prob = random.random()\n","        for i in range(len(iters)):\n","            if i == test_index and len(iters) != 1 and accept_prob > 0.5:\n","                test.append([iters[i], label_tensor])\n","            else:\n","                train.append([iters[i], label_tensor])\n","    return train, test\n","\n","def kfoldSplit(pairs, lang, device, split=10):\n","    folds = []\n","    inputs = []\n","    outputs = []\n","    for label in pairs:\n","        for iter in pairs[label]:\n","            inputs.append(iter)\n","            outputs.append(label)\n","    \n","    skf = model_selection.StratifiedKFold(n_splits=split, shuffle=True)\n","    indices = skf.split(inputs, outputs)\n","\n","    for train_indices, test_indices in indices:\n","        curr_train = []\n","        curr_test = []\n","        for indices in train_indices:\n","            curr_train.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n","        for indices in test_indices:\n","            curr_test.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n","        folds.append([curr_train, curr_test])\n","    \n","    return folds\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMiOxmB3LvSl","colab_type":"text"},"source":["Accuracy calculator and result documentation"]},{"cell_type":"code","metadata":{"id":"FakNorerKgKs","colab_type":"code","colab":{}},"source":["import torch\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def evaluate(encoder, decoder, sentence, output_lang, sil0, sil1, max_input_length=470, max_output_length=5):\n","    with torch.no_grad():\n","        input_tensor = sentence\n","        input_length = len(sentence)\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0,0]\n","\n","        decoder_input = torch.tensor([[sil0]], device=device)\n","        decoder_attentions = torch.zeros(max_output_length, max_input_length)\n","\n","        decoder_hidden = encoder_hidden\n","        decoded_words = []\n","\n","        for di in range(max_output_length):\n","            decoder_output, decoder_hidden, decoder_attn = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.data.topk(1)\n","            decoder_attentions[di] = decoder_attn.data\n","            if topi.item() == sil1:\n","                decoded_words.append('sil1')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words, decoder_attentions[:di+1]\n","\n","def calculateTrainingAccuracy(encoder, decoder, pairs, output_lang, sil0, sil1, file_name=None, write = True, max_input_length=470, max_output_length=5):\n","    total = 0\n","    correct = 0\n","    results = None\n","    if write:\n","        results = open(file_name, 'w')\n","    attention = None\n","    for pair in pairs:\n","        output_words, attention = evaluate(encoder, decoder, pair[0], output_lang, sil0, sil1, max_input_length=max_input_length, max_output_length=max_output_length)\n","        output_sentence = ' '.join(output_words)\n","        sent = [output_lang.index2word[i.item()] for i in pair[1]]\n","        true_sentence = ' '.join(sent)\n","        if write:\n","            print('Predicted Sentence: ', output_sentence)\n","            print('True Sentence: ' , true_sentence)\n","            plt.matshow(attention.numpy())\n","            print('Predicted Sentence: ', output_sentence, file=results)\n","            print('True Sentence: ' , true_sentence, file=results)\n","        answer = None\n","        if output_sentence == true_sentence:\n","            correct += 1\n","            answer = \"CORRECT\"\n","        else:\n","            answer = \"INCORRECT\"\n","        total += 1\n","        if write:\n","            print('Result: ', answer, file=results)\n","    if write:\n","        print('Recognition Total: ', str(correct/total), file=results)\n","        results.close()\n","    return correct/total\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO-yavYcLefW","colab_type":"text"},"source":["LSTM training methods"]},{"cell_type":"code","metadata":{"id":"E-wTkIecKgNf","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import random \n","import time\n","import torch.optim as optim\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","import copy\n","\n","teacher_forcing_ratio = 0.5\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","    plt.show()\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length = 470):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = len(input_tensor)\n","    target_length = target_tensor.size(0)\n","\n","    loss = 0\n","\n","    encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0,0]\n","\n","    decoder_input = torch.tensor([[sil0]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, _ = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, _ = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == sil1:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length\n","\n","def testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1, max_input_length=470):\n","    with torch.no_grad():\n","        input_tensor = input_tensor\n","        input_length = len(input_tensor)\n","        target_length = target_tensor.size(0)\n","\n","        loss = 0\n","\n","        encoder_hidden = encoder.initHidden()\n","        encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0,0]\n","\n","        decoder_input = torch.tensor([[sil0]], device=device)\n","\n","        layers, batches, hidden_num = encoder_hidden.size()\n","        decoder_hidden = encoder_hidden\n","        \n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.data.topk(1)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if topi.item() == sil1:\n","                break\n","            decoder_input = topi.squeeze().detach()\n","\n","        return loss.item() / target_length\n","\n","def trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, output_lang, lr=1e-4, lr_decay=1, lr_drop_epoch=10, l2_penalty = 0, max_input_length=470, max_output_length = 6):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    test_loss_total = 0\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay = l2_penalty)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr, weight_decay = l2_penalty)\n","\n","    best_test_acc = -1\n","    best_encoder = None\n","    best_decoder = None\n","\n","    criterion = nn.NLLLoss()\n","\n","    drop = False #False  = piecewise drop. True = gradual drop.\n","\n","    for iter in range(1, epochs + 1):\n","        if drop:\n","            if iter == lr_drop_epoch:\n","                encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n","                decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n","        else:\n","            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n","            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n","\n","        for pairs in train_set:\n","            input_tensor = pairs[0]\n","            target_tensor = pairs[1]\n","            loss = train(input_tensor, target_tensor, encoder,\n","                        decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length=max_input_length)\n","            print_loss_total += loss\n","\n","        for pair in test_set:\n","            input_tensor = pair[0]\n","            target_tensor = pair[1]\n","            test_loss_total += testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1, max_input_length=max_input_length)\n","\n","        print_loss_avg = print_loss_total / len(train_set)\n","        test_loss_avg = test_loss_total / len(test_set)\n","        print_loss_total = 0\n","        test_loss_total = 0\n","        test_acc = calculateTrainingAccuracy(encoder, decoder, test_set, output_lang, sil0, sil1, write=False, max_input_length=max_input_length, max_output_length=max_output_length)\n","        train_acc = calculateTrainingAccuracy(encoder, decoder, train_set, output_lang, sil0, sil1, write=False, max_input_length=max_input_length, max_output_length=max_output_length)\n","        print('%s (%d %d%%) train loss: %.4f train acc: %.4f test loss: %.4f test acc: %.4f' % (timeSince(start, iter / epochs),\n","                                        iter, iter / epochs * 100, print_loss_avg, train_acc, test_loss_avg, test_acc))\n","        \n","        if test_acc > best_test_acc:\n","            best_test_acc = test_acc\n","            best_encoder = copy.deepcopy(encoder)\n","            best_decoder = copy.deepcopy(decoder)\n","\n","        plot_losses.append(test_loss_avg)\n","\n","    showPlot(plot_losses)\n","    return best_encoder, best_decoder"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owWXYW6vMFN6","colab_type":"text"},"source":["Main script - uses above files to run everything"]},{"cell_type":"code","metadata":{"id":"H8888m0AKgH5","colab_type":"code","outputId":"ae86a937-ba4e-414f-86b6-0163f5ea66c1","executionInfo":{"status":"error","timestamp":1589398998582,"user_tz":-330,"elapsed":6643698,"user":{"displayName":"DHRUVA BANSAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjozSR-DG0yT8nOlVOylFHoeIYpSXFFQsi3040ccw=s64","userId":"12303647104962841566"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import glob\n","import random\n","import math\n","\n","#########    HYPERPARAMETERS   ############\n","random.seed(42)\n","users = [\"Linda\"]\n","file_name = \"Linda\"\n","num_features = 0\n","hidden_size = 2400\n","epochs = 60\n","limit_features = False\n","lr = 1e-4\n","lr_decay = 0.95\n","lr_drop = 20\n","num_layers = 1\n","k_fold = False\n","folds = 5\n","expansion_factor = 2\n","l2_penalty = 0\n","dropout = 0\n","###########################################\n","\n","sil0 = 0\n","sil1 = 0\n","\n","def expand(dataset_as_array, factor):\n","    expanded_array = []\n","    for pair in dataset_as_array:\n","        content = pair[0]\n","        label = pair[1]\n","\n","        expanded_pair = [[[],label] for i in range(factor)]\n","        for frame in range(len(content)):\n","            expanded_pair[frame % factor][0].append(content[frame])\n","        expanded_array.extend(expanded_pair)\n","    return expanded_array\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eng = Lang(\"english\")\n","pairs = {}\n","max_input_length = 0\n","max_output_length = 0\n","print(\"Reading data from files...\")\n","for user in users:\n","    for file in glob.glob(\"data/\"+user+\"/*.ark\"):\n","        label = \"sil0_\"+file.split(\".\")[1]+\"_sil1\"\n","        label = label.replace(\"_\", \" \")\n","        eng.addSentence(label)\n","\n","        max_output_length = max(max_output_length, len(label.split(\" \")))\n","\n","        sil0 = eng.word2index[\"sil0\"]\n","        sil1 = eng.word2index[\"sil1\"]\n","        content = []\n","        f = open(file)\n","        for x in f:\n","            line = x\n","            if \"[\" in x:\n","                line = x.split(\"[ \")[1]\n","            elif \"]\" in x:\n","                line = x.split(\"]\")[0]\n","            features = []\n","            line = line.strip(\"\\n\").split(\" \")\n","            if limit_features:\n","                line = line[-num_features:]\n","            for f in line:\n","                try:\n","                    features.append(float(f)*1000)\n","                except:\n","                    pass\n","            if len(features) != 0:\n","                num_features = len(features)\n","                content.append(torch.tensor(features, dtype=torch.float, device=device).view(1, 1, -1))\n","        max_input_length = max(max_input_length, math.ceil(len(content)/expansion_factor))\n","        if label in pairs:\n","            temp = pairs[label]\n","            temp.append(content)\n","            pairs[label] = temp\n","        else:\n","            pairs[label] = [content]\n","\n","print(\"Max Input length = \"+str(max_input_length) + \" Max output length = \" + str(max_output_length))\n","\n","for label in pairs:\n","    print(\"Label = \" + label + \" Number of iterations = \" + str(len(pairs[label])))\n","\n","# max_output_length = max_input_length\n","if not k_fold:    \n","    print(\"Splitting data into train and test...\")\n","    train_set, test_set = split(pairs, eng, device)\n","    train_set, test_set = expand(train_set, expansion_factor), expand(test_set, expansion_factor)\n","    encoder = EncoderGRU(num_features, hidden_size, dropout=dropout).to(device)\n","    decoder = AttnDecoderGRU(hidden_size, eng.n_words, dropout=dropout, max_input_length=max_input_length).to(device)\n","    print(\"Split done. Elements in train: %d and elements in test: %d. Starting training...\" % (len(train_set), len(test_set)))\n","    best_encoder, best_decoder = trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty, max_input_length=max_input_length, max_output_length=max_output_length)\n","    print(\"Training done. Printing stats to file....\")\n","    calculateTrainingAccuracy(best_encoder, best_decoder, test_set, eng, sil0, sil1, 'results/'+file_name+'/results.txt', max_input_length=max_input_length, max_output_length=max_output_length)\n","    print(\"Saving Models\")\n","    torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM.pt\")\n","    torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM.pt\")\n","\n","else:\n","    print(\"Generating folds...\")\n","    trainTestFolds = kfoldSplit(pairs, eng, device, split=folds)\n","    print(\"Fold generation done...\")\n","    fold_num = 1\n","    for curr_fold in trainTestFolds:\n","        encoder = EncoderGRU(num_features, hidden_size, dropout=dropout).to(device)\n","        decoder = AttnDecoderGRU(hidden_size, eng.n_words, dropout=dropout, max_input_length=max_input_length).to(device)\n","        print(\"Starting training on fold %d. %d elements in curr_fold[0] and %d in curr_fold[1]\" % (fold_num, len(curr_fold[0]), len(curr_fold[1])))\n","        best_encoder, best_decoder = trainIters(encoder, decoder, epochs, curr_fold[0], curr_fold[1], sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty, max_input_length=max_input_length, max_output_length=max_output_length)\n","        print(\"Training done. Saving predictions to file...\")\n","        calculateTrainingAccuracy(best_encoder, best_decoder, curr_fold[1], eng, sil0, sil1, 'results/'+file_name+'/results_fold'+str(fold_num)+'.txt')\n","        print(\"Saving Models\")\n","        torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM_fold\"+str(fold_num)+\".pt\")\n","        torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM_fold\"+str(fold_num)+\".pt\")\n","        fold_num += 1\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Reading data from files...\n","Max Input length = 148 Max output length = 7\n","Label = sil0 monkey in grey box sil1 Number of iterations = 4\n","Label = sil0 snake below black chair sil1 Number of iterations = 4\n","Label = sil0 snake below chair sil1 Number of iterations = 4\n","Label = sil0 monkey above wall sil1 Number of iterations = 4\n","Label = sil0 white lion above orange wall sil1 Number of iterations = 5\n","Label = sil0 white lion in grey box sil1 Number of iterations = 4\n","Label = sil0 lion above flowers sil1 Number of iterations = 5\n","Label = sil0 monkey in blue box sil1 Number of iterations = 5\n","Label = sil0 monkey below blue chair sil1 Number of iterations = 4\n","Label = sil0 lion above grey wall sil1 Number of iterations = 6\n","Label = sil0 monkey above white wall sil1 Number of iterations = 5\n","Label = sil0 orange alligator in grey flowers sil1 Number of iterations = 4\n","Label = sil0 monkey in box sil1 Number of iterations = 5\n","Label = sil0 snake in flowers sil1 Number of iterations = 5\n","Label = sil0 lion below orange chair sil1 Number of iterations = 5\n","Label = sil0 lion above wall sil1 Number of iterations = 6\n","Label = sil0 lion below chair sil1 Number of iterations = 4\n","Label = sil0 snake below blue flowers sil1 Number of iterations = 6\n","Label = sil0 lion below blue bed sil1 Number of iterations = 5\n","Label = sil0 monkey below wagon sil1 Number of iterations = 5\n","Label = sil0 snake above box sil1 Number of iterations = 5\n","Label = sil0 snake below bed sil1 Number of iterations = 5\n","Label = sil0 orange monkey below grey flowers sil1 Number of iterations = 7\n","Label = sil0 snake below blue chair sil1 Number of iterations = 5\n","Label = sil0 white snake in blue flowers sil1 Number of iterations = 5\n","Label = sil0 snake in grey wagon sil1 Number of iterations = 5\n","Label = sil0 white alligator above blue wall sil1 Number of iterations = 4\n","Label = sil0 monkey below bed sil1 Number of iterations = 5\n","Label = sil0 lion above orange bed sil1 Number of iterations = 5\n","Label = sil0 monkey above chair sil1 Number of iterations = 5\n","Label = sil0 orange monkey in grey box sil1 Number of iterations = 5\n","Label = sil0 orange snake below blue flowers sil1 Number of iterations = 5\n","Label = sil0 monkey in orange flowers sil1 Number of iterations = 5\n","Label = sil0 snake above wall sil1 Number of iterations = 5\n","Label = sil0 alligator above bed sil1 Number of iterations = 5\n","Label = sil0 alligator above black wall sil1 Number of iterations = 6\n","Label = sil0 alligator above blue wagon sil1 Number of iterations = 4\n","Label = sil0 alligator above blue wall sil1 Number of iterations = 5\n","Label = sil0 alligator above chair sil1 Number of iterations = 5\n","Label = sil0 alligator above orange wagon sil1 Number of iterations = 5\n","Label = sil0 alligator above wall sil1 Number of iterations = 5\n","Label = sil0 alligator below grey bed sil1 Number of iterations = 4\n","Label = sil0 alligator in orange flowers sil1 Number of iterations = 5\n","Label = sil0 alligator in box sil1 Number of iterations = 5\n","Label = sil0 alligator in wagon sil1 Number of iterations = 5\n","Label = sil0 black alligator above orange wagon sil1 Number of iterations = 5\n","Label = sil0 black lion above grey bed sil1 Number of iterations = 5\n","Label = sil0 black lion in blue wagon sil1 Number of iterations = 5\n","Label = sil0 black monkey in white flowers sil1 Number of iterations = 4\n","Label = sil0 black snake below blue chair sil1 Number of iterations = 6\n","Label = sil0 blue alligator above grey wall sil1 Number of iterations = 4\n","Label = sil0 blue monkey above grey box sil1 Number of iterations = 3\n","Label = sil0 grey alligator below blue flowers sil1 Number of iterations = 5\n","Label = sil0 grey monkey below orange chair sil1 Number of iterations = 5\n","Label = sil0 grey snake below blue chair sil1 Number of iterations = 5\n","Label = sil0 lion above bed sil1 Number of iterations = 5\n","Label = sil0 lion above blue bed sil1 Number of iterations = 6\n","Label = sil0 lion above box sil1 Number of iterations = 6\n","Splitting data into train and test...\n","Split done. Elements in train: 508 and elements in test: 60. Starting training...\n","2m 34s (- 152m 1s) (1 1%) train loss: 1.7458 train acc: 0.0276 test loss: 2.7799 test acc: 0.0333\n","5m 8s (- 149m 18s) (2 3%) train loss: 1.4821 train acc: 0.0197 test loss: 3.9685 test acc: 0.0333\n","7m 44s (- 147m 0s) (3 5%) train loss: 1.3284 train acc: 0.0197 test loss: 3.2050 test acc: 0.0333\n","10m 18s (- 144m 21s) (4 6%) train loss: 1.2620 train acc: 0.0197 test loss: 3.4918 test acc: 0.0333\n","12m 53s (- 141m 53s) (5 8%) train loss: 1.1851 train acc: 0.0197 test loss: 2.8621 test acc: 0.0333\n","15m 29s (- 139m 28s) (6 10%) train loss: 1.1155 train acc: 0.0394 test loss: 3.0595 test acc: 0.0333\n","18m 5s (- 136m 56s) (7 11%) train loss: 1.0917 train acc: 0.0433 test loss: 2.3355 test acc: 0.0333\n","20m 40s (- 134m 23s) (8 13%) train loss: 1.0774 train acc: 0.0472 test loss: 2.3147 test acc: 0.0333\n","23m 15s (- 131m 50s) (9 15%) train loss: 1.0213 train acc: 0.0433 test loss: 2.4878 test acc: 0.0333\n","25m 51s (- 129m 16s) (10 16%) train loss: 1.0474 train acc: 0.0472 test loss: 2.3035 test acc: 0.0333\n","28m 26s (- 126m 42s) (11 18%) train loss: 0.9917 train acc: 0.0492 test loss: 2.3314 test acc: 0.0333\n","31m 2s (- 124m 10s) (12 20%) train loss: 1.0141 train acc: 0.0472 test loss: 2.7019 test acc: 0.0333\n","33m 38s (- 121m 36s) (13 21%) train loss: 1.0110 train acc: 0.0492 test loss: 2.3840 test acc: 0.0333\n","36m 13s (- 119m 1s) (14 23%) train loss: 1.0043 train acc: 0.0472 test loss: 2.1706 test acc: 0.0667\n","38m 49s (- 116m 27s) (15 25%) train loss: 0.9602 train acc: 0.0630 test loss: 2.2615 test acc: 0.0333\n","41m 24s (- 113m 52s) (16 26%) train loss: 0.9450 train acc: 0.0610 test loss: 2.2196 test acc: 0.0333\n","44m 0s (- 111m 17s) (17 28%) train loss: 0.8991 train acc: 0.0551 test loss: 2.4118 test acc: 0.0333\n","46m 35s (- 108m 43s) (18 30%) train loss: 0.9215 train acc: 0.0650 test loss: 2.3072 test acc: 0.0833\n","49m 11s (- 106m 8s) (19 31%) train loss: 0.8867 train acc: 0.0689 test loss: 2.2920 test acc: 0.0667\n","51m 47s (- 103m 34s) (20 33%) train loss: 0.9606 train acc: 0.0630 test loss: 2.5089 test acc: 0.1000\n","54m 22s (- 100m 59s) (21 35%) train loss: 0.9259 train acc: 0.0827 test loss: 2.1609 test acc: 0.0833\n","56m 58s (- 98m 24s) (22 36%) train loss: 0.8772 train acc: 0.0787 test loss: 2.2607 test acc: 0.1167\n","59m 34s (- 95m 50s) (23 38%) train loss: 0.8558 train acc: 0.0866 test loss: 2.3304 test acc: 0.0500\n","62m 10s (- 93m 15s) (24 40%) train loss: 0.9724 train acc: 0.0906 test loss: 2.1584 test acc: 0.0833\n","64m 45s (- 90m 40s) (25 41%) train loss: 0.9687 train acc: 0.0906 test loss: 2.1031 test acc: 0.0833\n","67m 22s (- 88m 5s) (26 43%) train loss: 0.9305 train acc: 0.0768 test loss: 2.3630 test acc: 0.0167\n","69m 58s (- 85m 31s) (27 45%) train loss: 0.9157 train acc: 0.0925 test loss: 2.3553 test acc: 0.0833\n","72m 34s (- 82m 56s) (28 46%) train loss: 0.9175 train acc: 0.1043 test loss: 2.5403 test acc: 0.0667\n","75m 10s (- 80m 21s) (29 48%) train loss: 0.8559 train acc: 0.1043 test loss: 2.5558 test acc: 0.0500\n","77m 45s (- 77m 45s) (30 50%) train loss: 0.9095 train acc: 0.1063 test loss: 2.3510 test acc: 0.1167\n","80m 22s (- 75m 10s) (31 51%) train loss: 0.8815 train acc: 0.1083 test loss: 2.7071 test acc: 0.0667\n","82m 57s (- 72m 35s) (32 53%) train loss: 0.8753 train acc: 0.1122 test loss: 2.4820 test acc: 0.0333\n","85m 33s (- 70m 0s) (33 55%) train loss: 0.8643 train acc: 0.0965 test loss: 2.1226 test acc: 0.0667\n","88m 9s (- 67m 25s) (34 56%) train loss: 0.8518 train acc: 0.1063 test loss: 2.5989 test acc: 0.0667\n","90m 45s (- 64m 49s) (35 58%) train loss: 0.8708 train acc: 0.1240 test loss: 2.6224 test acc: 0.0333\n","93m 22s (- 62m 14s) (36 60%) train loss: 0.8661 train acc: 0.1319 test loss: 2.6630 test acc: 0.1000\n","95m 58s (- 59m 39s) (37 61%) train loss: 0.8638 train acc: 0.1339 test loss: 2.7284 test acc: 0.0833\n","98m 35s (- 57m 4s) (38 63%) train loss: 0.9145 train acc: 0.1358 test loss: 2.4221 test acc: 0.1000\n","101m 11s (- 54m 29s) (39 65%) train loss: 0.8060 train acc: 0.1378 test loss: 2.4875 test acc: 0.0500\n","103m 47s (- 51m 53s) (40 66%) train loss: 0.8213 train acc: 0.1417 test loss: 2.4017 test acc: 0.1333\n","106m 22s (- 49m 17s) (41 68%) train loss: 0.8441 train acc: 0.1358 test loss: 2.5604 test acc: 0.0333\n","108m 59s (- 46m 42s) (42 70%) train loss: 0.8747 train acc: 0.1240 test loss: 2.7276 test acc: 0.0667\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2c08f7f9addb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_input_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Split done. Elements in train: %d and elements in test: %d. Starting training...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mbest_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_drop_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_input_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_output_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done. Printing stats to file....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcalculateTrainingAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msil1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/results.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_input_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_output_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-9338aeedba9f>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, epochs, train_set, test_set, sil0, sil1, output_lang, lr, lr_decay, lr_drop_epoch, l2_penalty, max_input_length, max_output_length)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m--> 149\u001b[0;31m                         decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length=max_input_length)\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-9338aeedba9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1a4054eeccf4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 727\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}