{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CopyCat_LSTM_Training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"v5eNYJdhNMlG","colab_type":"text"},"source":["Basic Setup\n"]},{"cell_type":"code","metadata":{"id":"nFbaOUiWKrTB","colab_type":"code","outputId":"cd4d6b83-4bd2-4287-a5e7-f39e9d31db0a","executionInfo":{"status":"ok","timestamp":1589465042246,"user_tz":-330,"elapsed":53649,"user":{"displayName":"AVNI BANSAL","photoUrl":"","userId":"06936794149863007583"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/LSTM/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/LSTM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iw6d7m5MKJfW","colab_type":"code","outputId":"26d85bc0-d8b4-42e0-af48-24a5098fa32f","executionInfo":{"status":"ok","timestamp":1589465047342,"user_tz":-330,"elapsed":58722,"user":{"displayName":"AVNI BANSAL","photoUrl":"","userId":"06936794149863007583"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["pip install torch torchvision\n","import torch\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zekYo16sP1Qr","colab_type":"text"},"source":["Language Class"]},{"cell_type":"code","metadata":{"id":"GTDU_GvvPzf0","colab_type":"code","colab":{}},"source":["class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {}\n","        self.n_words = 0\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrsUvZv5Kjkd","colab_type":"text"},"source":["Encoder LSTM setup"]},{"cell_type":"code","metadata":{"id":"MfH2NvaTKfu7","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch\n","\n","class EncoderLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout, num_layers=1, bidirectional=False):\n","        super(EncoderLSTM, self).__init__()\n","        self.bidirectional = bidirectional\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n","\n","    def forward(self, input, hidden, cell):\n","        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n","        return output, hidden, cell\n","\n","    def initHidden(self, batch=1, num_layers=1):\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        return torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device), torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51DsXOu5Knnq","colab_type":"text"},"source":["Decoder LSTM setup"]},{"cell_type":"code","metadata":{"id":"KWlJs8ZdKf5U","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class DecoderLSTM(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout, num_layers=1, bidirectional=False):\n","        super(DecoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(output_size, self.hidden_size)\n","        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=self.bidirectional)\n","        self.out = nn.Linear(self.hidden_size * (1 + self.bidirectional), output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden, cell):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, (hidden, cell) = self.lstm(output, (hidden, cell))\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden, cell\n","\n","    def initHidden(self, batch=1):\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        return torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNmd0iM_LVcG","colab_type":"text"},"source":["Cross - Validation Fold generator"]},{"cell_type":"code","metadata":{"id":"q3eoOWNAKgAA","colab_type":"code","colab":{}},"source":["import random\n","import torch\n","from sklearn import model_selection\n","\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence, device):\n","    indexes = indexesFromSentence(lang, sentence)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def split(pairs, lang, device):\n","    train = []\n","    test = []\n","    for label in pairs:\n","        label_tensor = tensorFromSentence(lang, label, device)\n","        iters = pairs[label]\n","        test_index = random.randint(0, len(iters) - 1)\n","        accept_prob = random.random()\n","        for i in range(len(iters)):\n","            if i == test_index and len(iters) != 1 and accept_prob > 0.5:\n","                test.append([iters[i], label_tensor])\n","            else:\n","                train.append([iters[i], label_tensor])\n","    return train, test\n","\n","def kfoldSplit(pairs, lang, device, split=10):\n","    folds = []\n","    inputs = []\n","    outputs = []\n","    for label in pairs:\n","        for iter in pairs[label]:\n","            inputs.append(iter)\n","            outputs.append(label)\n","    \n","    skf = model_selection.StratifiedKFold(n_splits=split, shuffle=True)\n","    indices = skf.split(inputs, outputs)\n","\n","    for train_indices, test_indices in indices:\n","        curr_train = []\n","        curr_test = []\n","        for indices in train_indices:\n","            curr_train.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n","        for indices in test_indices:\n","            curr_test.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n","        folds.append([curr_train, curr_test])\n","    \n","    return folds\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMiOxmB3LvSl","colab_type":"text"},"source":["Accuracy calculator and result documentation"]},{"cell_type":"code","metadata":{"id":"FakNorerKgKs","colab_type":"code","colab":{}},"source":["import torch\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def evaluate(encoder, decoder, sentence, output_lang, sil0, sil1, max_length=470):\n","    with torch.no_grad():\n","        input_tensor = sentence\n","        input_length = len(sentence)\n","        encoder_hidden, encoder_cell = encoder.initHidden()\n","\n","\n","        for ei in range(input_length):\n","            _, encoder_hidden, encoder_cell = encoder(input_tensor[ei],\n","                                                     encoder_hidden, encoder_cell)\n","\n","        decoder_input = torch.tensor([[sil0]], device=device)\n","\n","        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n","        \n","        decoded_words = []\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_cell = decoder(\n","                decoder_input, decoder_hidden, decoder_cell)\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == sil1:\n","                decoded_words.append('sil1')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words\n","\n","def calculateTrainingAccuracy(encoder, decoder, pairs, output_lang, sil0, sil1, file_name=None, write = True):\n","    total = 0\n","    correct = 0\n","    results = None\n","    if write:\n","        results = open(file_name, 'w')\n","    for pair in pairs:\n","        output_words = evaluate(encoder, decoder, pair[0], output_lang, sil0, sil1)\n","        output_sentence = ' '.join(output_words)\n","        sent = [output_lang.index2word[i.item()] for i in pair[1]]\n","        true_sentence = ' '.join(sent)\n","        if write:\n","            print('Predicted Sentence: ', output_sentence, file=results)\n","            print('True Sentence: ' , true_sentence, file=results)\n","        answer = None\n","        if output_sentence == true_sentence:\n","            correct += 1\n","            answer = \"CORRECT\"\n","        else:\n","            answer = \"INCORRECT\"\n","        total += 1\n","        if write:\n","            print('Result: ', answer, file=results)\n","    if write:\n","        print('Recognition Total: ', str(correct/total), file=results)\n","        results.close()\n","    return correct/total\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO-yavYcLefW","colab_type":"text"},"source":["LSTM training methods"]},{"cell_type":"code","metadata":{"id":"E-wTkIecKgNf","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import random \n","import time\n","import torch.optim as optim\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","import copy\n","\n","teacher_forcing_ratio = 0.5\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","    plt.show()\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1):\n","    encoder_hidden, encoder_cell = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = len(input_tensor)\n","    target_length = target_tensor.size(0)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        _, encoder_hidden, encoder_cell = encoder(\n","            input_tensor[ei], encoder_hidden, encoder_cell)\n","\n","    decoder_input = torch.tensor([[sil0]], device=device)\n","\n","    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_cell = decoder(\n","                decoder_input, decoder_hidden, decoder_cell)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_cell = decoder(\n","                decoder_input, decoder_hidden, decoder_cell)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == sil1:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length\n","\n","def testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1):\n","    with torch.no_grad():\n","        input_tensor = input_tensor\n","        input_length = len(input_tensor)\n","        target_length = target_tensor.size(0)\n","\n","        loss = 0\n","\n","        encoder_hidden, encoder_cell = encoder.initHidden()\n","        for ei in range(input_length):\n","            _, encoder_hidden, encoder_cell = encoder(input_tensor[ei],\n","                                                     encoder_hidden, encoder_cell)\n","\n","        decoder_input = torch.tensor([[sil0]], device=device)\n","\n","        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n","        \n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_cell = decoder(\n","                decoder_input, decoder_hidden, decoder_cell)\n","            topv, topi = decoder_output.data.topk(1)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if topi.item() == sil1:\n","                break\n","            decoder_input = topi.squeeze().detach()\n","\n","        return loss.item() / target_length\n","\n","def trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, output_lang, lr=1e-4, lr_decay=1, lr_drop_epoch=10, l2_penalty = 0):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    test_loss_total = 0\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay = l2_penalty)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr, weight_decay = l2_penalty)\n","\n","    best_test_acc = -1\n","    best_encoder = None\n","    best_decoder = None\n","\n","    criterion = nn.NLLLoss()\n","\n","    drop = True\n","    for iter in range(1, epochs + 1):\n","        if drop:\n","            if iter == lr_drop_epoch:\n","                encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n","                decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n","        else:\n","            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n","            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n","\n","        for pairs in train_set:\n","            input_tensor = pairs[0]\n","            target_tensor = pairs[1]\n","            loss = train(input_tensor, target_tensor, encoder,\n","                        decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1)\n","            print_loss_total += loss\n","\n","        for pair in test_set:\n","            input_tensor = pair[0]\n","            target_tensor = pair[1]\n","            test_loss_total += testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1)\n","\n","        print_loss_avg = print_loss_total / len(train_set)\n","        test_loss_avg = test_loss_total / len(test_set)\n","        print_loss_total = 0\n","        test_loss_total = 0\n","        test_acc = calculateTrainingAccuracy(encoder, decoder, test_set, output_lang, sil0, sil1, write=False)\n","        train_acc = calculateTrainingAccuracy(encoder, decoder, train_set, output_lang, sil0, sil1, write=False)\n","        print('%s (%d %d%%) train loss: %.4f train acc: %.4f test loss: %.4f test acc: %.4f' % (timeSince(start, iter / epochs),\n","                                        iter, iter / epochs * 100, print_loss_avg, train_acc, test_loss_avg, test_acc))\n","        \n","        if test_acc > best_test_acc:\n","            best_test_acc = test_acc\n","            best_encoder = copy.deepcopy(encoder)\n","            best_decoder = copy.deepcopy(decoder)\n","\n","        plot_losses.append(test_loss_avg)\n","\n","    showPlot(plot_losses)\n","    return best_encoder, best_decoder"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owWXYW6vMFN6","colab_type":"text"},"source":["Main script - uses above files to run everything"]},{"cell_type":"code","metadata":{"id":"H8888m0AKgH5","colab_type":"code","outputId":"d3415b16-088f-447d-ccf8-1ffcee4653da","executionInfo":{"status":"ok","timestamp":1587857914826,"user_tz":-330,"elapsed":9439171,"user":{"displayName":"DHRUVA BANSAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjozSR-DG0yT8nOlVOylFHoeIYpSXFFQsi3040ccw=s64","userId":"12303647104962841566"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import glob\n","import random\n","\n","#########    HYPERPARAMETERS   ############\n","random.seed(42)\n","users = [\"Linda\"]\n","file_name = \"Linda\"\n","num_features = 0\n","hidden_size = 1500\n","epochs = 60\n","limit_features = False\n","lr = 1e-4\n","lr_decay = 0.95\n","lr_drop = 20\n","dropout = 0.5\n","num_layers = 1\n","k_fold = False\n","folds = 5\n","bidirectional = True\n","expansion_factor = 1\n","l2_penalty = 0\n","###########################################\n","\n","sil0 = 0\n","sil1 = 0\n","\n","def expand(dataset_as_array, factor):\n","    expanded_array = []\n","    for pair in dataset_as_array:\n","        content = pair[0]\n","        label = pair[1]\n","\n","        expanded_pair = [[[],label] for i in range(factor)]\n","        for frame in range(len(content)):\n","            expanded_pair[frame % factor][0].append(content[frame])\n","        expanded_array.extend(expanded_pair)\n","    return expanded_array\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eng = Lang(\"english\")\n","pairs = {}\n","print(\"Reading data from files...\")\n","for user in users:\n","    for file in glob.glob(\"data/\"+user+\"/*.ark\"):\n","        label = \"sil0_\"+file.split(\".\")[1]+\"_sil1\"\n","        label = label.replace(\"_\", \" \")\n","        eng.addSentence(label)\n","\n","        sil0 = eng.word2index[\"sil0\"]\n","        sil1 = eng.word2index[\"sil1\"]\n","        content = []\n","        f = open(file)\n","        for x in f:\n","            line = x\n","            if \"[\" in x:\n","                line = x.split(\"[ \")[1]\n","            elif \"]\" in x:\n","                line = x.split(\"]\")[0]\n","            features = []\n","            line = line.strip(\"\\n\").split(\" \")\n","            if limit_features:\n","                line = line[-num_features:]\n","            for f in line:\n","                try:\n","                    features.append(float(f)*1000)\n","                except:\n","                    pass\n","            if len(features) != 0:\n","                num_features = len(features)\n","                content.append(torch.tensor(features, dtype=torch.float, device=device).view(1, 1, -1))\n","        if label in pairs:\n","            temp = pairs[label]\n","            temp.append(content)\n","            pairs[label] = temp\n","        else:\n","            pairs[label] = [content]\n","for label in pairs:\n","    print(\"Label = \" + label + \" Number of iterations = \" + str(len(pairs[label])))\n","if not k_fold:    \n","    print(\"Splitting data into train and test...\")\n","    train_set, test_set = split(pairs, eng, device)\n","    train_set, test_set = expand(train_set, expansion_factor), expand(test_set, expansion_factor)\n","    encoder = EncoderLSTM(num_features, hidden_size, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n","    decoder = DecoderLSTM(hidden_size, eng.n_words, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n","    print(\"Split done. Elements in train: %d and elements in test: %d. Starting training...\" % (len(train_set), len(test_set)))\n","    best_encoder, best_decoder = trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty)\n","    print(\"Training done. Printing stats to file....\")\n","    calculateTrainingAccuracy(best_encoder, best_decoder, test_set, eng, sil0, sil1, 'results/'+file_name+'/results.txt')\n","    print(\"Saving Models\")\n","    torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM.pt\")\n","    torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM.pt\")\n","\n","else:\n","    print(\"Generating folds...\")\n","    trainTestFolds = kfoldSplit(pairs, eng, device, split=folds)\n","    print(\"Fold generation done...\")\n","    fold_num = 1\n","    for curr_fold in trainTestFolds:\n","        encoder = EncoderLSTM(num_features, hidden_size, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n","        decoder = DecoderLSTM(hidden_size, eng.n_words, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n","        print(\"Starting training on fold %d. %d elements in curr_fold[0] and %d in curr_fold[1]\" % (fold_num, len(curr_fold[0]), len(curr_fold[1])))\n","        best_encoder, best_decoder = trainIters(encoder, decoder, epochs, curr_fold[0], curr_fold[1], sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty)\n","        print(\"Training done. Saving predictions to file...\")\n","        calculateTrainingAccuracy(best_encoder, best_decoder, curr_fold[1], eng, sil0, sil1, 'results/'+file_name+'/results_fold'+str(fold_num)+'.txt')\n","        print(\"Saving Models\")\n","        torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM_fold\"+str(fold_num)+\".pt\")\n","        torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM_fold\"+str(fold_num)+\".pt\")\n","        fold_num += 1\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading data from files...\n","Label = sil0 monkey in grey box sil1 Number of iterations = 4\n","Label = sil0 snake below black chair sil1 Number of iterations = 4\n","Label = sil0 snake below chair sil1 Number of iterations = 4\n","Label = sil0 monkey above wall sil1 Number of iterations = 4\n","Label = sil0 white lion above orange wall sil1 Number of iterations = 5\n","Label = sil0 white lion in grey box sil1 Number of iterations = 4\n","Label = sil0 lion above flowers sil1 Number of iterations = 5\n","Label = sil0 monkey in blue box sil1 Number of iterations = 5\n","Label = sil0 monkey below blue chair sil1 Number of iterations = 4\n","Label = sil0 lion above grey wall sil1 Number of iterations = 6\n","Label = sil0 monkey above white wall sil1 Number of iterations = 5\n","Label = sil0 orange alligator in grey flowers sil1 Number of iterations = 4\n","Label = sil0 monkey in box sil1 Number of iterations = 5\n","Label = sil0 snake in flowers sil1 Number of iterations = 5\n","Label = sil0 lion below orange chair sil1 Number of iterations = 5\n","Label = sil0 lion above wall sil1 Number of iterations = 6\n","Label = sil0 lion below chair sil1 Number of iterations = 4\n","Label = sil0 snake below blue flowers sil1 Number of iterations = 6\n","Label = sil0 lion below blue bed sil1 Number of iterations = 5\n","Label = sil0 monkey below wagon sil1 Number of iterations = 5\n","Label = sil0 snake above box sil1 Number of iterations = 5\n","Label = sil0 snake below bed sil1 Number of iterations = 5\n","Label = sil0 orange monkey below grey flowers sil1 Number of iterations = 7\n","Label = sil0 snake below blue chair sil1 Number of iterations = 5\n","Label = sil0 white snake in blue flowers sil1 Number of iterations = 5\n","Label = sil0 snake in grey wagon sil1 Number of iterations = 5\n","Label = sil0 white alligator above blue wall sil1 Number of iterations = 4\n","Label = sil0 monkey below bed sil1 Number of iterations = 5\n","Label = sil0 lion above orange bed sil1 Number of iterations = 5\n","Label = sil0 monkey above chair sil1 Number of iterations = 5\n","Label = sil0 orange monkey in grey box sil1 Number of iterations = 5\n","Label = sil0 orange snake below blue flowers sil1 Number of iterations = 5\n","Label = sil0 monkey in orange flowers sil1 Number of iterations = 5\n","Label = sil0 snake above wall sil1 Number of iterations = 5\n","Label = sil0 alligator above bed sil1 Number of iterations = 5\n","Label = sil0 alligator above black wall sil1 Number of iterations = 6\n","Label = sil0 alligator above blue wagon sil1 Number of iterations = 4\n","Label = sil0 alligator above blue wall sil1 Number of iterations = 5\n","Label = sil0 alligator above chair sil1 Number of iterations = 5\n","Label = sil0 alligator above orange wagon sil1 Number of iterations = 5\n","Label = sil0 alligator above wall sil1 Number of iterations = 5\n","Label = sil0 alligator below grey bed sil1 Number of iterations = 4\n","Label = sil0 alligator in orange flowers sil1 Number of iterations = 5\n","Label = sil0 alligator in box sil1 Number of iterations = 5\n","Label = sil0 alligator in wagon sil1 Number of iterations = 5\n","Label = sil0 black alligator above orange wagon sil1 Number of iterations = 5\n","Label = sil0 black lion above grey bed sil1 Number of iterations = 5\n","Label = sil0 black lion in blue wagon sil1 Number of iterations = 5\n","Label = sil0 black monkey in white flowers sil1 Number of iterations = 4\n","Label = sil0 black snake below blue chair sil1 Number of iterations = 6\n","Label = sil0 blue alligator above grey wall sil1 Number of iterations = 4\n","Label = sil0 blue monkey above grey box sil1 Number of iterations = 3\n","Label = sil0 grey alligator below blue flowers sil1 Number of iterations = 5\n","Label = sil0 grey monkey below orange chair sil1 Number of iterations = 5\n","Label = sil0 grey snake below blue chair sil1 Number of iterations = 5\n","Label = sil0 lion above bed sil1 Number of iterations = 5\n","Label = sil0 lion above blue bed sil1 Number of iterations = 6\n","Label = sil0 lion above box sil1 Number of iterations = 6\n","Splitting data into train and test...\n","Split done. Elements in train: 254 and elements in test: 30. Starting training...\n","2m 14s (- 132m 4s) (1 1%) train loss: 1.8593 train acc: 0.0000 test loss: 1.6991 test acc: 0.0000\n","4m 29s (- 130m 7s) (2 3%) train loss: 1.4918 train acc: 0.0157 test loss: 1.9061 test acc: 0.0000\n","6m 44s (- 128m 4s) (3 5%) train loss: 1.4570 train acc: 0.0157 test loss: 1.7512 test acc: 0.0000\n","8m 59s (- 125m 58s) (4 6%) train loss: 1.3600 train acc: 0.0236 test loss: 1.7615 test acc: 0.0000\n","11m 14s (- 123m 43s) (5 8%) train loss: 1.3937 train acc: 0.0276 test loss: 1.5655 test acc: 0.0000\n","13m 29s (- 121m 29s) (6 10%) train loss: 1.2842 train acc: 0.0472 test loss: 1.6996 test acc: 0.0000\n","15m 44s (- 119m 14s) (7 11%) train loss: 1.2533 train acc: 0.0394 test loss: 1.5687 test acc: 0.0000\n","17m 59s (- 116m 57s) (8 13%) train loss: 1.2778 train acc: 0.0276 test loss: 1.5691 test acc: 0.0000\n"],"name":"stdout"}]}]}