{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CopyCat_LSTM_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5eNYJdhNMlG",
        "colab_type": "text"
      },
      "source": [
        "Basic Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw6d7m5MKJfW",
        "colab_type": "code",
        "outputId": "7c7698c6-8a58-49c9-95dd-d0656a9be5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "pip install torch torchvision\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFbaOUiWKrTB",
        "colab_type": "code",
        "outputId": "e9751f1a-1c7f-4dc1-bf59-cc187570a5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import torch\n",
        "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/LSTM/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zekYo16sP1Qr",
        "colab_type": "text"
      },
      "source": [
        "Language Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTDU_GvvPzf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrsUvZv5Kjkd",
        "colab_type": "text"
      },
      "source": [
        "Encoder LSTM setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfH2NvaTKfu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout, num_layers=1, bidirectional=False):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self, batch=1, num_layers=1):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device), torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51DsXOu5Knnq",
        "colab_type": "text"
      },
      "source": [
        "Decoder LSTM setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWlJs8ZdKf5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout, num_layers=1, bidirectional=False):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=self.bidirectional)\n",
        "        self.out = nn.Linear(self.hidden_size * (1 + self.bidirectional), output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self, batch=1):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return torch.zeros(self.num_layers * (1 + self.bidirectional), batch, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNmd0iM_LVcG",
        "colab_type": "text"
      },
      "source": [
        "Cross - Validation Fold generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3eoOWNAKgAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from sklearn import model_selection\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence, device):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def split(pairs, lang, device):\n",
        "    train = []\n",
        "    test = []\n",
        "    for label in pairs:\n",
        "        label_tensor = tensorFromSentence(lang, label, device)\n",
        "        iters = pairs[label]\n",
        "        test_index = random.randint(0, len(iters) - 1)\n",
        "        accept_prob = random.random()\n",
        "        for i in range(len(iters)):\n",
        "            if i == test_index and len(iters) != 1 and accept_prob > 0.5:\n",
        "                test.append([iters[i], label_tensor])\n",
        "            else:\n",
        "                train.append([iters[i], label_tensor])\n",
        "    return train, test\n",
        "\n",
        "def kfoldSplit(pairs, lang, device, split=10):\n",
        "    folds = []\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for label in pairs:\n",
        "        for iter in pairs[label]:\n",
        "            inputs.append(iter)\n",
        "            outputs.append(label)\n",
        "    \n",
        "    skf = model_selection.StratifiedKFold(n_splits=split, shuffle=True)\n",
        "    indices = skf.split(inputs, outputs)\n",
        "\n",
        "    for train_indices, test_indices in indices:\n",
        "        curr_train = []\n",
        "        curr_test = []\n",
        "        for indices in train_indices:\n",
        "            curr_train.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n",
        "        for indices in test_indices:\n",
        "            curr_test.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n",
        "        folds.append([curr_train, curr_test])\n",
        "    \n",
        "    return folds\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMiOxmB3LvSl",
        "colab_type": "text"
      },
      "source": [
        "Accuracy calculator and result documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FakNorerKgKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, output_lang, sil0, sil1, max_length=470):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence\n",
        "        input_length = len(sentence)\n",
        "        encoder_hidden, encoder_cell = encoder.initHidden()\n",
        "\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            _, encoder_hidden, encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden, encoder_cell)\n",
        "\n",
        "        decoder_input = torch.tensor([[sil0]], device=device)\n",
        "\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "        \n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == sil1:\n",
        "                decoded_words.append('sil1')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "def calculateTrainingAccuracy(encoder, decoder, pairs, output_lang, sil0, sil1, file_name=None, write = True):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    results = None\n",
        "    if write:\n",
        "        results = open(file_name, 'w')\n",
        "    for pair in pairs:\n",
        "        output_words = evaluate(encoder, decoder, pair[0], output_lang, sil0, sil1)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        sent = [output_lang.index2word[i.item()] for i in pair[1]]\n",
        "        true_sentence = ' '.join(sent)\n",
        "        if write:\n",
        "            print('Predicted Sentence: ', output_sentence, file=results)\n",
        "            print('True Sentence: ' , true_sentence, file=results)\n",
        "        answer = None\n",
        "        if output_sentence == true_sentence:\n",
        "            correct += 1\n",
        "            answer = \"CORRECT\"\n",
        "        else:\n",
        "            answer = \"INCORRECT\"\n",
        "        total += 1\n",
        "        if write:\n",
        "            print('Result: ', answer, file=results)\n",
        "    if write:\n",
        "        print('Recognition Total: ', str(correct/total), file=results)\n",
        "        results.close()\n",
        "    return correct/total\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO-yavYcLefW",
        "colab_type": "text"
      },
      "source": [
        "LSTM training methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-wTkIecKgNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random \n",
        "import time\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1):\n",
        "    encoder_hidden, encoder_cell = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = len(input_tensor)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        _, encoder_hidden, encoder_cell = encoder(\n",
        "            input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "\n",
        "    decoder_input = torch.tensor([[sil0]], device=device)\n",
        "\n",
        "    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == sil1:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = input_tensor\n",
        "        input_length = len(input_tensor)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        encoder_hidden, encoder_cell = encoder.initHidden()\n",
        "        for ei in range(input_length):\n",
        "            _, encoder_hidden, encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden, encoder_cell)\n",
        "\n",
        "        decoder_input = torch.tensor([[sil0]], device=device)\n",
        "\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "        \n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if topi.item() == sil1:\n",
        "                break\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return loss.item() / target_length\n",
        "\n",
        "def trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, output_lang, lr=1e-4, lr_decay=1, lr_drop_epoch=10, l2_penalty = 0):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    test_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay = l2_penalty)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr, weight_decay = l2_penalty)\n",
        "\n",
        "    best_test_acc = -1\n",
        "    best_encoder = None\n",
        "    best_decoder = None\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, epochs + 1):\n",
        "        if iter == lr_drop_epoch:\n",
        "            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n",
        "            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n",
        "\n",
        "        for pairs in train_set:\n",
        "            input_tensor = pairs[0]\n",
        "            target_tensor = pairs[1]\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1)\n",
        "            print_loss_total += loss\n",
        "\n",
        "        for pair in test_set:\n",
        "            input_tensor = pair[0]\n",
        "            target_tensor = pair[1]\n",
        "            test_loss_total += testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1)\n",
        "\n",
        "        print_loss_avg = print_loss_total / len(train_set)\n",
        "        test_loss_avg = test_loss_total / len(test_set)\n",
        "        print_loss_total = 0\n",
        "        test_loss_total = 0\n",
        "        test_acc = calculateTrainingAccuracy(encoder, decoder, test_set, output_lang, sil0, sil1, write=False)\n",
        "        train_acc = calculateTrainingAccuracy(encoder, decoder, train_set, output_lang, sil0, sil1, write=False)\n",
        "        print('%s (%d %d%%) train loss: %.4f train acc: %.4f test loss: %.4f test acc: %.4f' % (timeSince(start, iter / epochs),\n",
        "                                        iter, iter / epochs * 100, print_loss_avg, train_acc, test_loss_avg, test_acc))\n",
        "        \n",
        "        if test_acc > best_test_acc:\n",
        "            bet_test_acc = test_acc\n",
        "            best_encoder = encoder\n",
        "            best_decoder = decoder\n",
        "\n",
        "        plot_loss_avg = print_loss_avg\n",
        "        plot_losses.append(plot_loss_avg)\n",
        "\n",
        "    # showPlot(plot_losses)\n",
        "    return best_encoder, best_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owWXYW6vMFN6",
        "colab_type": "text"
      },
      "source": [
        "Main script - uses above files to run everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8888m0AKgH5",
        "colab_type": "code",
        "outputId": "2b43cd1d-e128-4f98-fd9c-f0666074818b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import random\n",
        "\n",
        "#########    HYPERPARAMETERS   ############\n",
        "random.seed(42)\n",
        "users = [\"Ravi\"]\n",
        "file_name = \"Ravi\"\n",
        "num_features = 0\n",
        "hidden_size = 1024\n",
        "epochs = 60\n",
        "limit_features = False\n",
        "lr = 1e-4\n",
        "lr_decay = 0.95\n",
        "lr_drop = 15\n",
        "dropout = 0.5\n",
        "num_layers = 1\n",
        "k_fold = False\n",
        "folds = 5\n",
        "bidirectional = False\n",
        "expansion_factor = 2\n",
        "l2_penalty = 0.001\n",
        "###########################################\n",
        "\n",
        "sil0 = 0\n",
        "sil1 = 0\n",
        "\n",
        "def expand(dataset_as_array, factor):\n",
        "    expanded_array = []\n",
        "    for pair in dataset_as_array:\n",
        "        content = pair[0]\n",
        "        label = pair[1]\n",
        "\n",
        "        expanded_pair = [[[],label] for i in range(factor)]\n",
        "        for frame in range(len(content)):\n",
        "            expanded_pair[frame % factor][0].append(content[frame])\n",
        "        expanded_array.extend(expanded_pair)\n",
        "    return expanded_array\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "eng = Lang(\"english\")\n",
        "pairs = {}\n",
        "print(\"Reading data from files...\")\n",
        "for user in users:\n",
        "    for file in glob.glob(\"data/\"+user+\"/*.ark\"):\n",
        "        label = \"sil0_\"+file.split(\".\")[1]+\"_sil1\"\n",
        "        label = label.replace(\"_\", \" \")\n",
        "        eng.addSentence(label)\n",
        "\n",
        "        sil0 = eng.word2index[\"sil0\"]\n",
        "        sil1 = eng.word2index[\"sil1\"]\n",
        "        content = []\n",
        "        f = open(file)\n",
        "        for x in f:\n",
        "            line = x\n",
        "            if \"[\" in x:\n",
        "                line = x.split(\"[ \")[1]\n",
        "            elif \"]\" in x:\n",
        "                line = x.split(\"]\")[0]\n",
        "            features = []\n",
        "            line = line.strip(\"\\n\").split(\" \")\n",
        "            if limit_features:\n",
        "                line = line[-num_features:]\n",
        "            for f in line:\n",
        "                try:\n",
        "                    features.append(float(f)*1000)\n",
        "                except:\n",
        "                    pass\n",
        "            if len(features) != 0:\n",
        "                num_features = len(features)\n",
        "                content.append(torch.tensor(features, dtype=torch.float, device=device).view(1, 1, -1))\n",
        "        if label in pairs:\n",
        "            temp = pairs[label]\n",
        "            temp.append(content)\n",
        "            pairs[label] = temp\n",
        "        else:\n",
        "            pairs[label] = [content]\n",
        "if not k_fold:    \n",
        "    print(\"Splitting data into train and test...\")\n",
        "    train_set, test_set = split(pairs, eng, device)\n",
        "    train_set, test_set = expand(train_set, expansion_factor), expand(test_set, expansion_factor)\n",
        "    encoder = EncoderLSTM(num_features, hidden_size, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "    decoder = DecoderLSTM(hidden_size, eng.n_words, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "    print(\"Split done. Elements in train: %d and elements in test: %d. Starting training...\" % (len(train_set), len(test_set)))\n",
        "    best_encoder, best_decoder = trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty)\n",
        "    print(\"Training done. Printing stats to file....\")\n",
        "    calculateTrainingAccuracy(best_encoder, best_decoder, test_set, eng, sil0, sil1, 'results/'+file_name+'/results.txt')\n",
        "    print(\"Saving Models\")\n",
        "    torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM.pt\")\n",
        "    torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM.pt\")\n",
        "\n",
        "else:\n",
        "    print(\"Generating folds...\")\n",
        "    trainTestFolds = kfoldSplit(pairs, eng, device, split=folds)\n",
        "    print(\"Fold generation done...\")\n",
        "    fold_num = 1\n",
        "    for curr_fold in trainTestFolds:\n",
        "        encoder = EncoderLSTM(num_features, hidden_size, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "        decoder = DecoderLSTM(hidden_size, eng.n_words, dropout, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "        print(\"Starting training on fold %d. %d elements in curr_fold[0] and %d in curr_fold[1]\" % (fold_num, len(curr_fold[0]), len(curr_fold[1])))\n",
        "        best_encoder, best_decoder = trainIters(encoder, decoder, epochs, curr_fold[0], curr_fold[1], sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty)\n",
        "        print(\"Training done. Saving predictions to file...\")\n",
        "        calculateTrainingAccuracy(best_encoder, best_decoder, curr_fold[1], eng, sil0, sil1, 'results/'+file_name+'/results_fold'+str(fold_num)+'.txt')\n",
        "        print(\"Saving Models\")\n",
        "        torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM_fold\"+str(fold_num)+\".pt\")\n",
        "        torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM_fold\"+str(fold_num)+\".pt\")\n",
        "        fold_num += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data from files...\n",
            "Splitting data into train and test...\n",
            "Split done. Elements in train: 300 and elements in test: 56. Starting training...\n",
            "0m 20s (- 19m 57s) (1 1%) train loss: 1.8947 train acc: 0.0000 test loss: 1.7628 test acc: 0.0000\n",
            "0m 40s (- 19m 41s) (2 3%) train loss: 1.5080 train acc: 0.0000 test loss: 1.9428 test acc: 0.0000\n",
            "1m 1s (- 19m 21s) (3 5%) train loss: 1.4073 train acc: 0.0500 test loss: 1.7260 test acc: 0.0357\n",
            "1m 21s (- 19m 1s) (4 6%) train loss: 1.3185 train acc: 0.0367 test loss: 1.6670 test acc: 0.0000\n",
            "1m 41s (- 18m 41s) (5 8%) train loss: 1.2397 train acc: 0.0700 test loss: 1.6253 test acc: 0.0000\n",
            "2m 2s (- 18m 20s) (6 10%) train loss: 1.1785 train acc: 0.0833 test loss: 1.5553 test acc: 0.0000\n",
            "2m 22s (- 18m 0s) (7 11%) train loss: 1.1376 train acc: 0.0867 test loss: 1.6484 test acc: 0.0000\n",
            "2m 43s (- 17m 40s) (8 13%) train loss: 1.0673 train acc: 0.1167 test loss: 1.5831 test acc: 0.0357\n",
            "3m 3s (- 17m 20s) (9 15%) train loss: 1.0290 train acc: 0.1567 test loss: 1.4993 test acc: 0.0536\n",
            "3m 24s (- 17m 0s) (10 16%) train loss: 0.9847 train acc: 0.1700 test loss: 1.5742 test acc: 0.0714\n",
            "3m 44s (- 16m 40s) (11 18%) train loss: 0.9244 train acc: 0.1600 test loss: 1.5893 test acc: 0.0714\n",
            "4m 5s (- 16m 20s) (12 20%) train loss: 0.9161 train acc: 0.2000 test loss: 1.5222 test acc: 0.0714\n",
            "4m 25s (- 16m 0s) (13 21%) train loss: 0.8737 train acc: 0.2000 test loss: 1.5505 test acc: 0.0357\n",
            "4m 46s (- 15m 39s) (14 23%) train loss: 0.8105 train acc: 0.3133 test loss: 1.3545 test acc: 0.1071\n",
            "5m 6s (- 15m 19s) (15 25%) train loss: 0.6956 train acc: 0.4000 test loss: 1.3915 test acc: 0.0893\n",
            "5m 26s (- 14m 59s) (16 26%) train loss: 0.6398 train acc: 0.4733 test loss: 1.2681 test acc: 0.1429\n",
            "5m 47s (- 14m 38s) (17 28%) train loss: 0.6014 train acc: 0.5033 test loss: 1.3082 test acc: 0.1071\n",
            "6m 7s (- 14m 18s) (18 30%) train loss: 0.5468 train acc: 0.4800 test loss: 1.3034 test acc: 0.0893\n",
            "6m 28s (- 13m 58s) (19 31%) train loss: 0.4950 train acc: 0.5900 test loss: 1.2769 test acc: 0.1429\n",
            "6m 48s (- 13m 37s) (20 33%) train loss: 0.4804 train acc: 0.5933 test loss: 1.2472 test acc: 0.1607\n",
            "7m 9s (- 13m 17s) (21 35%) train loss: 0.4074 train acc: 0.6533 test loss: 1.2646 test acc: 0.2321\n",
            "7m 29s (- 12m 56s) (22 36%) train loss: 0.3913 train acc: 0.6867 test loss: 1.1608 test acc: 0.2321\n",
            "7m 50s (- 12m 36s) (23 38%) train loss: 0.3358 train acc: 0.7367 test loss: 1.2389 test acc: 0.1964\n",
            "8m 10s (- 12m 16s) (24 40%) train loss: 0.2930 train acc: 0.7733 test loss: 1.2962 test acc: 0.2143\n",
            "8m 31s (- 11m 55s) (25 41%) train loss: 0.2748 train acc: 0.8167 test loss: 1.2365 test acc: 0.3036\n",
            "8m 51s (- 11m 35s) (26 43%) train loss: 0.2108 train acc: 0.8700 test loss: 1.2132 test acc: 0.3036\n",
            "9m 12s (- 11m 15s) (27 45%) train loss: 0.1913 train acc: 0.8967 test loss: 1.3510 test acc: 0.2143\n",
            "9m 33s (- 10m 54s) (28 46%) train loss: 0.1500 train acc: 0.9167 test loss: 1.2698 test acc: 0.2500\n",
            "9m 53s (- 10m 34s) (29 48%) train loss: 0.1225 train acc: 0.9167 test loss: 1.4701 test acc: 0.1964\n",
            "10m 13s (- 10m 13s) (30 50%) train loss: 0.1334 train acc: 0.9133 test loss: 1.3284 test acc: 0.3036\n",
            "10m 34s (- 9m 53s) (31 51%) train loss: 0.1126 train acc: 0.9467 test loss: 1.4312 test acc: 0.1964\n",
            "10m 55s (- 9m 33s) (32 53%) train loss: 0.0776 train acc: 0.9733 test loss: 1.3733 test acc: 0.2857\n",
            "11m 15s (- 9m 12s) (33 55%) train loss: 0.0634 train acc: 0.9867 test loss: 1.4245 test acc: 0.3036\n",
            "11m 36s (- 8m 52s) (34 56%) train loss: 0.0497 train acc: 0.9733 test loss: 1.3661 test acc: 0.2857\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}